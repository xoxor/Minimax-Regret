\newif\ifappendix
%\appendixtrue %COMMENT to remove appendix % UNCOMMENT to add apendix
\input{ijcai_preamble} 
\author{ID: 2696}
%\author{Beatrice Napolitano$^1$\and Olivier Cailloux$^1$	\and  Paolo Viappiani$^{2}$
%	\affiliations $^1$ LAMSADE, UMR 7243, CNRS and Universit\'e Paris Dauphine, PSL Research University, Paris, France\\ $^2$ LIP6, UMR 7606, CNRS and Sorbonne Universit\'e, Paris, France
%	\emails \{firstname.lastname\}@dauphine.fr, paolo.viappiani@lip6.fr
%}
%Restoring page numbers as missing them out violates basic human rights of the reviewers. -- Olivier
\thispagestyle{plain}
\pagestyle{plain}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\newunicodechar{⇒}{\implies}
\newunicodechar{≠}{\ensuremath{\neq}}
\newunicodechar{≤}{\leq}
\newunicodechar{≥}{\geq}
%… Horizontal Ellipsis
\DeclareUnicodeCharacter{2026}{\ifmmode\dots\else\textellipsis\fi}
\newunicodechar{∧}{\land}
\newunicodechar{∨}{\lor}
\newunicodechar{∩}{\cap}
\newunicodechar{∪}{\cup}
%¬ Not Sign
\DeclareUnicodeCharacter{00AC}{\ifmmode\lnot\else\textlnot\fi}
\newunicodechar{⇔}{\Leftrightarrow}
\newcommand{\N}{ℕ}
\newunicodechar{ℕ}{\mathbb{N}}
\usepackage{algorithm, algpseudocode}
\usepackage{amsmath,amssymb,enumerate,amsthm}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{mathrsfs}

\usepackage{bm}
\usepackage{empheq}
\usepackage{cleveref}
\usepackage{xcolor}

\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\newcommand{\denselist}{\itemsep -2pt\topsep-6pt\partopsep-6pt}
\newcommand{\commentOC}[1]{\textcolor{blue}{\small$\big[$OC: #1$\big]$}}

\newcommand{\pref}{\succ}%real, connected pref, strict
\newcommand{\prefr}{{\succ}^\text{r}}%real, connected pref, strict
\newcommand{\ppreflarge}{\succeq^\text{p}}%partial pref
\newcommand{\ppref}{\succ^\text{p}}%partial pref
\newcommand{\pprefinv}{\prec^\text{p}}%partial pref
\newcommand{\nppref}{\nsucc^\text{p}}%negated partial pref
\newcommand{\linors}{\mathcal{L}(A)}
%https://tex.stackexchange.com/a/45732, works within both \set and \set*, same spacing than \mid (https://tex.stackexchange.com/a/52905).
\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}

%Thanks to https://tex.stackexchange.com/q/154549
\makeatletter
\newcommand{\newrelation}[2]{% #1 = control sequence, #2 = replacement text
  \@ifdefinable{#1}{%
    \def#1{%
    \@ifnextchar_{\csname\string#1\endcsname}{\mathrel{#2}}%
    }%
    \@namedef{\string#1}##1##2{\mathrel{#2_{##2}}}%
  }%
}
\makeatother

\newrelation{\pinc}{\!\parallel\!}%partial pref, complement (incomparable)
%\newrelation{\pinc}{Q^\text{p}}%partial pref, complement (incomparable)

\newcommand{\profile}{\bm{v}}%(complete) profile
\newcommand{\pprofile}{{\bm{p}}}%partial profile
\newcommand{\w}{\bm{w}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Co}{\mathcal{C}}
\newcommand{\pw}{W}%our knowledge about the weights
\newcommand{\powersetz}[1]{\mathscr{P}^*(#1)}

\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\SCORE}{Score}
\DeclareMathOperator{\PMR}{PMR}
\DeclareMathOperator{\MR}{MR}
\DeclareMathOperator{\MMR}{MMR}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{claim}{Claim}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclarePairedDelimiter\set{\{}{\}}
\DeclarePairedDelimiter\card{\lvert}{\rvert}

%TODO remove this before submission
	\tolerance=2000
	%Accept overfull hbox up to...
	\hfuzz=1cm
	%Reduces verbosity about the bad line breaks.
	\hbadness 10000
	%Reduces verbosity about the underful vboxes.
	\vbadness=10000

\title{Robust Winner Determination and Simultaneous Elicitation of Scoring Rules and Preferences}

\begin{document}
\maketitle
\begin{abstract}
Social choice deals with the problem of determining a consensus choice from the preferences of different agents (voters).
In the classical setting, the voting rule is fixed beforehand and full preference information is provided.
Recently, the assumption of full preference information has been questioned by a number of researchers and several methods for eliciting preferences have been proposed.

In this paper we go one step further and we assume that both the voting rule and the agents preferences are partially specified.
Focusing on positional scoring rules, we assume that the chair, while  not able to give a precise definition of the rule, is capable of answering simple questions  requiring to pick a winner from a specific  example profile.
Moreover, the preferences of the agents are incrementally acquired by asking comparison queries.

We propose a method  for robust approximate winner determination in this setting with minimax regret. 
We then provide an interactive elicitation protocol based on minimax regret
and develop several query strategies that interleave questions to the chair and questions to the agents in order to acquire the most relevant information in order to quickly converge to optimal or a near-optimal alternative.
\end{abstract}

\section{Introduction}
%\commentOC{Do we agree that we should talk systematically about agents (or systematically about voters)? Currently we talk about voters in first section, then agents.}
Aggregation of preference information is a central task in many computer systems (recommender systems, search engines, etc).
In many situations, such as in group recommender systems, the goal is to find a consensus choice.
It is therefore natural to look at methods from social choice and see how they can be adapted for group decision-making in a computerized setting.

The traditional approach to social choice assumes that both the social function and the full preference orderings of the voters are expressed beforehand. This represents two strong hypothesis.
About the second one, requiring voters to express full preference orderings can be prohibitively costly (in term of cognitive and communication cost), especially for decisions with large sets of alternatives.
This observation has motivated a number of recent works considering social choice with partial preference orders  \citep{Xia2008,Pini2009,Konczak05} and incremental elicitation \citep{Kalech2011, Lu2011, Naamani-Dery2015} of voters’ preferences. 

The first hypothesis also should be relaxed, for in several situations it may not be easy to precisely define the voting rule.
Indeed, it is possible that the chair may have some preferences over the desired aggregation method, but may still be not able to formalize the voting rule upfront.
The work of \citet{Cailloux2014} provides elicitation methods for a quite general class of rules based on weak orders.
When considering positional scoring rules, several authors \citep{Stein1994, Llamazares2013, Viappiani2018} have worked on positional scoring rules with uncertain weights, assuming that the preferences of the voters are fully known.
%it is possible to derive dominance relations (akin to stochastic dominance) that allow to eliminate some alternatives since they will be less preferred than another one for any instantiation of the weights \citep{Stein1994}.
%Among others, Llamazares and Pe{\~{n}}a have considered  the problem of dealing with underspecified weights in positional scoring rules.

In this paper we consider that both the agents’ preferences and the social choice rule are partially specified.
We develop methods for computing the minimax-optimal alternative for scoring rules with partial agent preference information and partial information about the weights of the scoring rule.
While previous works have considered either partial information about the agents preferences or a partially specified aggregation method, we do not know of any work considering both sources of uncertainty at the same time.

In this work, we focus on positional scoring rules, that  are a particularly common method used to aggregate rankings and determine a winner.
We also address the problem of elicitation, providing incremental elicitation methods to acquire relevant preference information; we discuss  several heuristics that determine queries that quickly reduce minimax regret.
In particular, our query strategies focus simultaneously on reduction of relevant preference and voting rule uncertainty.
% the value of certain preference information is often not worth the cost of obtaining it.

%In this paper we propose an interactive adaptive protocol for eliciting both the agents preferences and the voting rule.
%At each step we need to decide whether we want to ask a question to the committee or to one of the agents (and to which agent in particular).
The paper is organized as follows.
In Section \ref{sec:background} we provide the necessary background.
We introduce the minimax criterion, that selects a winner that minimizes the worst possible regret, in Section \ref{sec:mmr}.
In Section \ref{sec:elicit} we provide an interactive elicitation protocol based on minimal regret;  in Section \ref{sec:experiments} we present the empirical validation of our approach with simulations; and Section \ref{sec:conclusions} provides some final thoughts.

\section{Social choice with partial information}
\label{sec:background}
We now introduce some basic concepts.
We assume defined a set $A$ of $m$ alternatives (products, restaurants, movies, public projects, job candidates, etc.) and a set $\set{1, …, n}$ of agents (voters); each agent $j$ is associated to her “real” preference order $\pref_j$. The agents come from an infinite set $\N$ of potential agents.
The real preferences of the agents are elements of $\linors$, the linear orders (connected, transitive, asymmetric relations) over the alternatives.
Following the social choice nomenclature, we call {\em profile} the association of a preference to each agent, considering a subset of agents from the set $\N$, and denote a profile by $(\pref_1,\ldots,\pref_n)$.
A profile is equivalently represented by $\profile=(v_1,\ldots,v_n)$ where $v_j(i) \in \set{1, \ldots, m}$ denotes the rank (position) of alternative $i$ in the preference order $\pref_j$. 

Let $V$ be the set of possible preference profiles (the union for any integer $n$ of the $n$-fold cartesian product of the linear orders over the alternatives).
A social choice function $f : V \rightarrow \powersetz{A}$ associates a profile with a set of winners, where $\powersetz{A}$ represents the set of subsets of $A$ except for the emptyset. (Sets are used for tied winners.)
Among the many possible social choice functions, we consider {\em positional scoring rules (PSR)}, which attach weights to positions according to  the vector $(w_1, \ldots, w_m)$ (also called the scoring vector).
%A scoring rule associates each alternative to a score that is given by the sum of points obtained for each voter.
An alternative obtains a score that depends on the rank obtained in each of the preference orders:
\begin{align}
\label{eq:srule}
s(x; \profile, \w) = \sum_{j=1}^{n} w_{v_j(x)}
= \sum_{r=1}^{m} \alpha^{x}_r w_r 
\end{align}
where $\alpha^{x}_r$ is the number of times that alternative $x$ was ranked in the $r$-th position.
The winners are the alternatives with highest scores.
%We assume that the weights constitute a monotonic sequence: $w_{1} \geq w_{2} \geq \ldots \geq w_{m}$.

In this work we assume fixed, but unknown, a profile $\profile^*$ (and corresponding agents preferences $\pref_j^*$) and a weight vector $\w^*$, and we want to reason about partial preference information concerning those objects.
Our knowledge at a given time of the preference of voter $j$ is encoded by a partial order over the alternatives, thus a transitive and asymmetric binary relation, denoted by $\ppref_j$. 
In this work we assume that preference information is truthful, i.e. $a \ppref_j b ⇒ a \pref_j^* b$.
We use $\pinc$ to denote incomparability, that is $a \pinc_{j} b$ iff $a \nppref_j b \wedge b \nppref_j a$.
An incomplete profile $\pprofile = (\ppref_1, \ldots, \ppref_n)$ maps each voter to a partial preference.

A completion of $\ppref_j$ is any linear order $\pref$ that extends $\ppref_j$.
Let $C(\ppref_j) = \set{{\succ} \in \linors \suchthat {\ppref_j} \subseteq {\succ}}$ be the set of possible completions of $\ppref_i$.
We let $C(\pprofile)=C(\ppref_1)\times … \times C(\ppref_n)$ be the set of complete profiles extending $p$. Note that $\profile^* \in C(\pprofile)$.

\medskip
We also assume that the weights of the scoring rule are only partially specified.
Therefore the vector $(w_1,\ldots,w_m)$  is not known but we are given a set of constraints restraining the possible values that weights can take.
We consider a decreasing sequence of weights\footnote{We assume all positions from $1$ to $m-1$ may contribute positively to the score of an alternative. The case where it is known that only the first $k<m-1$ positions matter can be easily dealt with by specifying $w_r = 0$ for $r > k$ as constraints. }:
% If the chair considers that, for example, “only the first ten positions matter” (so that $w_r = 0$ for $r > 10$), then we simply assume the chair is able to express this constraint at the start of the querying process, thus, that this information is known, and approximate the problem by considering that it is only about the first ten alternatives, thus, set $m=10$.}
\begin{align}
1=w_{1} > w_{2} > \ldots > w_{m}=0. \label{eq:monotone}
\end{align}
This is a natural assumption, as it is better to be ranked first than second, second than third, etc. 
Without loss of generality, we consider that $w_1=1$ and $w_m=0$. 
%\commentOC{I suggest we say no more than this to justify this assumption. Otherwise, we might say something like the following. 

The weights of a scoring rule can model different preferences of the chair. 
For instance, the weights can control the inclination to favor ``extreme'' alternatives (often at either the top or the bottom of the input rankings) at the expenses of ``moderate'' alternatives (that are more consistently in the middle part of the input rankings).

An important class of scoring rule is the one composed of weights that are a convex sequence \citep{Stein1994,Llamazares2016}, meaning that the difference between the weight of the first position and the weight of the second position is at least as great as the difference between the weights of the second and third positions, etc.
%\begin{align} \forall r \in \{1,\ldots,m-2\} \;\; & w_r - w_{r+1} \geq w_{r+1}-w_{r+2}  \\
%\iff & w_r - 2 w_{r+1} + w_{r+2} \geq 0 \label{eq:convexity}
%\end{align}
\begin{equation} 
\label{eq:convexity}
\forall r \in \{1,\ldots,m-2\}: w_r - w_{r+1} \geq w_{r+1}-w_{r+2}.
\end{equation}
The constraint above is often used when aggregating rankings in sport competitions.
We use $\W$ to denote the set of convex weight vectors.

In general it can be difficult to set the weights in an appropriate way.
We assume that in addition of basic requirements (monotonicity and convexity), the chair (the person or the organization that is supervising the voting process) is able to specify additional preferences about how the social choice function should behave.
In this work we assume that preferences of the chair are encoded with linear constraints about the vector $\w$, relating the value of the weights of different positions.
We use $\pw \subseteq \W$ to denote the set of weight vectors compatible with the preferences expressed by the chair about the scoring vector, and $\Co_W$ to denote the set of linear constraints expressed by the chair.

Of course it may be difficult for real decision makers to state preferences about the voting rule in such an abstract way.
These additional preferences can be elicited by asking questions about concrete profiles, for instance, by showing a complete profile of a small synthetic election and asking who should be elected in this case, as shown in Section \ref{sec:elicit}.

\section[Minimax regret under partial profile and weight information]{
Robust winner determination}
\label{sec:mmr}

%The quality of an alternative can be quantified by considering the maximum regret with respect to an adversary that can choose the instantiation of both a complete profile (extending the known preferences of the agents) and of the scoring vectors (associated to the preferences of the chair).
In this paper, we consider a setting where both the agents' preferences and the preferences of the chair about the voting rule are incomplete.
Notice that some authors have considered possible and necessary winners assuming a partial profile  \citep{Xia2008} or assuming an incompletely specified scoring rule \citep{Viappiani2018};
however, we note that, in typical settings, there will be no necessary winner and too many possible winners.
In practice, however, it is often useful to declare a winner given partial information.

As a decision criterion to determine a winner, we propose to use minimax regret. 
Minimax regret \citep{Savage1954} is a decision criterion that has been used for robust optimization under data uncertainty \citep{Kouvelis1997} as well as in decision-making with uncertain utility values \citep{Salo2001,Boutilier2006}.
\citet{Lu2011} have adopted minimax regret for winner determination in social choice with
the preferences of the agents that are only partially known, while the social choice function is predetermined and known.

In this work we consider the simultaneous presence of uncertainty in the agents' preferences and uncertainty in the weights.
Using {\em maximum regret} to quantify the worst-case error, the alternative that minimize this error is selected as winner, providing us with a form of robust optimization.
Intuitively, the quality of a proposed alternative $a$ is how far from optimal $a$  could be in the worst case, given the current knowledge about the voting rule and about the agents' preferences.

The maximum regret is considered by assuming that an adversary both 1) can extend the partial profile $\pprofile$ into a complete profile, and 2) can instantiate the weights choosing among any weight vector in $\pw$, where $\pprofile$ and $\pw$ represent our knowledge so far.
We formalize the notion of minimax regret in multiple steps.
First of all, $\Regret(x, \profile, \w)$ is the “regret” of selecting $x$ as a winner instead of choosing the optimal alternative under $\profile$ and $\w$:
\[\Regret(x, \profile, \w) = \max_{y \in A} s(y; \profile,\w) - s(x; \profile, \w).\]
The pairwise max regret $\PMR(x,y;\pprofile,W)$ of $x$ relative to $y$ given partial profile $\pprofile$ and the set of weights $W$
is the worst-case loss under all possible realizations of the full profile {\em and} all possible instantiations of the weights:
\begin{align}
\PMR(x,y; \pprofile, W) & = \max_{\w \in W} \max_{\profile \in C(\pprofile)} s(y; \profile,\w) - s(x; \profile,\w).
\end{align}

Max regret $\MR(x;\pprofile,W)$ is the worst-case loss of $x$. It is the loss occurred by an adversarial selection of a complete profile $\profile$ extending $\pprofile$ and a selection of $\w \in W$ to maximize the loss between $x$ and the true winner under $\profile$ and $\w$.
\begin{align}
\MR(x; \pprofile, W) & = \max_{y \in A} \PMR(x,y; \pprofile, W)\\
& = \max_{\w \in W} \max_{\profile \in C(\profile)} \Regret(x, \profile, \w).
\end{align}

Finally,  $\MMR(\pprofile,W)$ is the value of minimax regret under $\pprofile$ and $W$, obtained when recommending a minimax optimal alternative $x^*_{\pprofile, W}$:
\begin{align*}
\MMR(\pprofile,W) & = \min_{x \in A} \MR(x;\pprofile,W) \\
x^{*}_{\pprofile,W} \in A^*_{\pprofile, W} & = \argmin_{x \in A} \MR(x;\pprofile,W) 
\end{align*}
By picking as consensus choice
 an alternative associated with minimax regret, we can provide a recommendation that gives worst-case guarantees, giving some robustness in face of uncertainty (due to both not knowing the agents' preferences and the weights used in the aggregation). 
In cases of ties in minimax regret, we can either decide to return all minimax alternatives $A^*_{\pprofile, W}$ as winners or to pick just one of them using some tie-breaking strategy.

Notice that if $\MMR(\pprofile, W)\!=\!0$, then $x^{*}_{\pprofile,W}$  is a necessary co-winner; this means that for any valid completion of the profile and any feasible $w \!\in\! W$, $x^{*}_{\pprofile,W}$ obtains a highest score.

We note that our notion of regret gives some cardinal meaning to the scores: instead of just being used to select winners under the corresponding PSR, their differences are considered as representing the regret of the chair.

% Add some general remarks about using minimax regret

\subsection{Computation of minimax regret}
In order to compute pairwise maximum regret and therefore minimax regret, we adapt the reasoning from \citet{Lu2011} so that we decompose the $\PMR$ into the contributions associated to each agent.
The setting is however more challenging due to the presence of uncertainty in the weights.

Scoring rules are additively decomposable.
%A scoring rule associates each alternative to a score that is given by the sum of points obtained for each voter.
Recall that when computing $s(x; \profile, \w)$, $w_{v_j(x)}$ is the number of points that $x$ obtains in the ranking $v_j$ (see Equation \ref{eq:srule}).
By exploiting the decomposition of the score in terms of votes, we write the actual regret of choosing $x$ instead of $y$ as
\[
s(y; \profile,\w) - s(x; \profile, \w) = \sum_{j=1}^n w_{v_j(y)} - w_{v_j(x)},
\]
and  can rewrite $\PMR$ as follows:
\begin{align*}
& \PMR(x,y; \pprofile, W) = \max_{\w \in W} \max_{\profile \in C(\pprofile)} [ s(y; \profile,\w) - s(x; \profile,\w) ] \\
& =  \max_{\w \in W} \sum_{j=1}^{n} \max_{v_j \in C(\succ_j^p)} [w_{v_j(y)} - w_{v_j(x)}]. \\
\end{align*}
Note that in general the inner max will depend on the weights chosen by the outer min.

We consider now a procedure for completing a partial profile that was first proposed by \citet{Lu2011} when considering minimax regret with a fixed weight vector.
As we will show, this procedure can also be used in our case of uncertain weight vector.
%in the case of convex scoring rules we will be able to greatly simplify the problem by decoupling the two maximizations.

We are interested in computing $\PMR(x, y; \pprofile, W)$. This represents the “worst” difference of score, thus the difference of score between $y$ and $x$ under the worst case preferences compatible with $\pprofile$ and $W$, where the worst case is the one that maximizes this difference of score.
Consider our knowledge $\ppref_j$ of the preference of an agent $j$. 
The adversary has to make the score of $y$ as high as possible, and the score of $x$ as low as possible. 
To do this, he should complete $\ppref_j$ to $\pref_j$ by putting above $x$ as many alternatives as he can, that is, all the alternatives except those that are known to be worse than $x$ (those $a$ such that $x \ppref_j a$); and similarly, put below $y$ all the alternatives he can, thus, put above $y$ only those $a$ such that $a \ppref_j y$. 
Sometimes these two objectives conflict: when an alternative should go above $x$ according to the first objective and below $y$ according to the second one (because $¬(x \ppref_j a) ∧ ¬(a \ppref_j y)$), and $x$ is known to be better than $y$ (thus $x \ppref_j y$). 
In this case, the first objective should take priority, and $a$ should be put above $x$, which will move both $x$ and $y$ one rank lower than if $a$ had been put below $y$. 
This maximizes the adversary’s interests: because the weight vector is convex, the difference in scores will be lower when both alternatives are ranked lower (Equation \ref{eq:convexity}), and that difference of score is in favor of $x$ when $x \ppref_j y$, thus to be minimized from the point of view of the adversary.
To summarize:
\begin{align} 
\label{eq:compl}
\begin{split}
a \pref_j x &⇔ ¬(x \ppref_j a)\\
y \pref_j a &⇔ ¬(a \ppref_j y) ∧ (¬(x \ppref_j y) ∨ (x \ppref_j a)).
\end{split}
\end{align} 
In what follows let $\hat{\profile}$ be the profile computed according to the above procedure, and  $\hat{v}_j$ the $j$-th element (the linear order extending the partial order $p_j$ of the $j$-th voter).
We just sketched a proof of the following claim, and leave the rest of the proof to the reader.
\begin{claim}
There exists a completion $\hat{\profile} \in C(\pprofile)$ such that $\PMR(x,y; \pprofile, W) = \max_{\w \in W} [ s(y; \profile,\w) - s(x; \profile,\w) ]$ and that the linear order $\hat{v}_{j}$ of each agent $j$ satisfies (\ref{eq:compl}).
%There exists a completion ${\pref_j} \in C(\ppref_j)$ such that $\PMR(x,y; \pprofile, W) = \max_{\w \in W} \max_{\profile \in C(\pprofile), {\pref_j} \in \profile} [ s(y; \profile,\w) - s(x; \profile,\w) ]$ and that satisfies 
\end{claim}
%In this claim, we abuse notation by writing ${\pref_j} \in \profile$ to mean that the preference ordering corresponding to agent $j$ in $\profile$ is $\pref_j$.
\noindent It follows that $\hat{v}_{j}(x)$, the rank of $x$ for voter $j$ in the PMR-maximizing linear order $\bar{v}_{j}$ %$\PMR(x,y; \pprofile, W)$ 
is $\hat{v}_{j}(y) = 1+\card{A}-\card{{\ppref_j}(x)}$, where ${\ppref_j}(x)$ designates the set of alternatives that are less good than $x$ according to $\ppref_j$, and the rank of $y$ is $\hat{v}_{j}(x)=1+\card{{\pprefinv_j}(y)}+\beta$, where $\beta = \card{A} - \card{{\ppref_j}(x)}$ if $(x \ppref_j y)$ and $\beta = 0$ otherwise.

%Proof: Define $\pref$ as just $\ppref_j$ with the arcs added to minimally satisfy the above. Thus, add the following arcs: when $x ? a$, add $a > x$. When $a ? y ∧ x \ppref_j y ∧ ¬(x \ppref_j a)$, add $a \pref y$. When $a ? y ∧ [¬(x \ppref_j y) ∨ (x \ppref_j a)]$, add $y \pref a$. Observe that $\ppref_j \subseteq \pref$. We have to show that it is acyclic. (Then we take its transitive closure to make it a partial order, and it’s done.) Note that if there is a cycle in $\pref$, it’s because of some new arcs, as $\ppref_j$ is acyclic. Thus, the cycle must go through one of these new arcs. …

%Let $U^+=\{ i \mid \ y \ppref_i x\}$ be the set of agents for which we know that $y$ is preferred to $x$ (positive contribution to pairwise regret), $U^-=\{ i \mid x \ppref_i y\}$ 
%be the set of agents for wich we  know that $x$ is preferred to $y$ (negative contribution) and 
%$U^?=\{ i \mid x \pinc_i y\}$ 
%%$U^?=\{ i \mid x \nsucc_{i} y \wedge y \nsucc_{i} x \}$ 
%the remaining case, where the preference between $x$ and $y$ is not known.
%Consider how the adversary, whose goal is to maximize $\PMR$ between $x$ and $y$, will complete the partial orders:
%\begin{itemize}
% \item $j \in U^+$: If we know that agent $j$ prefers $y$ to $x$ then the contribution to $\PMR$ is positive and adversary will complete the partial order of agent $j$ by placing as many alternatives as possible between $y$ and $x$ (because of monotonicity of the weights).
% The way the partial order is completed does not depend on the actual values of the weights.
%
% \item $j \in U^?$: If we do not know whether $x$ or $y$ is preferred by agent $j$ then the adversary will place $y$ before $x$ (since the goal is to maximize the difference in score between $y$ and $x$, and weights are monotone) in the linear order and this case reduce to the first one; therefore also in this case the completion of the partial order can be done independently of the choice of $w$.
%
% \item $j \in U^-$: If we know that agent $j$ prefers $x$ to $y$ then (as the contribution of this agent to $\PMR$ is negative) the adversary will place as few alternatives as possible between $x$ and $y$.
%
%The only issue is with respect to the  alternatives that are incomparable with $x$ and $y$ according to $\ppref_i$: should they be placed better than $x$ or worse than $y$ ?
%If the scoring vector is known to be convex,  than these alternatives will be placed better than $x$ in the completion; otherwise the placements of these depend on the weights $\w$.
%\end{itemize}
%\subsubsection{Minimax regret computation}
%Since weights are assumed to be convex, the adversary will place all alternatives $c$ that are such that $c \pinc_{j} y \wedge c \pinc_{j} x$ as better than $x$.
%We now formalize the reasoning about the best (with respect to maximizing $\PMR$) completion with the following definition.
%\begin{definition}
%Given the partial profile $\pprofile$, define $\hat{\profile}$ as the completition of $\pprofile$ such that:
%\begin{itemize}
%\item For each $j \in U^{+}$, set   $\hat{\profile}$ as follows, for all $c \in A$:\\
%if $c \pinc_{j} x$ we set $c \succ_{j}^{\hat{\profile}} x$;\\
%if  $c \pinc_{j} y$ we set $y \succ_{j}^{\hat{\profile}} c$.
%\item For each $j \in U^{?}$, set  $\hat{\profile}$ as follows:\\
%$y \succ_{j}^{\hat{\profile}} x$;\\
%for all $c \in A$, if   $c \pinc_{j} x$ we set $c \succ_{j}^{\hat{\profile}} x$;\\
%for all $c \in A$, if   $c \pinc_{j} y$ we set $y \succ_{j}^{\hat{\profile}} c$.
%\item For each $j \in U^{-}$,  set  $\hat{\profile}$ as follows, for all $c \in A$:\\
%if $c \pinc_{j} x \wedge c \succ^{p}_{j} y$ we set $c \succ_{j}^{\hat{\profile}} x$;\\
%if $c \pinc_{j} y \wedge  x \succ^{p}_{j} c$ we set $y \succ_{j}^{\hat{\profile}} c$;\\
%if $c \pinc_{j} y \wedge c \pinc_{j} x$ we set $c \succ_{j}^{\hat{\profile}} x$.
%\end{itemize}
%\end{definition}

%Given our hypothesis, for all pairs of alternatives, there exist completions of the partial preferences $\ppref$ that maximize the PMR and that do not depend on the weights. 
%That is, we can exhibit, for each voter $j$, a completion $v_j$ such that $\forall \w, \forall v'_j \in C(\ppref_j): [w_{v_j(y)} - w_{v_j(x)}] \geq [w_{v'_j(y)} - w_{v'_j(x)}]$.

%In summary, some best completion $\pref$ satisfies two properties: (1) for all $c \pinc x$, $c \pref x$; and (2) for all $c \pinc y$, if not $c \pref x \pref y$ then $y \pref c$. 
%These two properties are intuitively understandable by observing that maximizing the PMR requires placing $x$ as low as possible, hence the first property; and $y$ as high as possible, hence the second property, with the exception that sometimes it is not possible to satisfy both goals and noting that the first goal is more important by convexity of the weights. 
%A proof that this reasoning is correct is in \cref{sec:prfCompl}.

\begin{claim}
The $\PMR$ can be written as:
\begin{align} 
\PMR(x,y; \pprofile, W)  
%\max_{w \in W} s(y; \hat{\profile}, \w) - s(y; \hat{\profile}, \w)\\
& = \max_{w \in W} \sum_{j=1}^n w_{\hat{v}_j(y)} - w_{\hat{v}_j(x)} = \\ 
& = \max_{w \in W} \sum_{r=1}^m (\hat{\alpha}_{r}^{y} - \hat{\alpha}_{r}^{x}) w_i. 
\end{align}
where the $\hat{\alpha}_{r}^{y}$ (resp. $\hat{\alpha}_{r}^{x}$)  is the number of times $y$ (resp. $x$) is at rank $r$ in the rankings of the completed profile $\hat{\profile}$.
%$\hat{\alpha}_{i}^{x} = \sum_{j=1}^{n}  I[\hat{v}_{j}(x)=i]$ where $I$ is the indicator function.
\end{claim}
%\[ \alpha_{i} = \sum_{j=1}^{n}  I[\hat{v}_{j}(y)=i] - I[\hat{v}_{j}(x)=i]\]
%We now show that the pairwise maximum regret can be computed with a linear program.
The PMR can be now computed by optimizing a linear program (LP) where the decision variables correspond to the weights attached to the different positions.
Constraints are on the feasible parameters (the weights of the scoring rules).
We remind that the preferences of the char are encoded with linear constraints $\Co_{W}$.
% contraints represent the preferences $\Co_{W}$ of the chair about the scoring rule.
The pairwise max regret $\PMR(x,y; \pprofile,W)$ is now obtained as the solution of the following linear program defined on the variables $w_1, …, w_m$. 
\begin{align*}
\max_{\w} & \sum_{r=1}^m (\hat{\alpha}_{r}^{y} - \hat{\alpha}_{r}^{x}) w_{r}\\
\text{ s.t. } & \text{Equation } (\ref{eq:monotone})\\
& \text{Equation } (\ref{eq:convexity})\\
& \Co_W.
\end{align*}
Note that given our choice $w_{1}=1$ and $w_{m}=0$, there are only $m-2$ variables 
(we leave $w_{1}$ and $w_{m}$ in the LP just for clarity of presentation).

%Finally the minimax regret solution $\MMR(\pprofile, W)$ is determined by solving a sequence of pairwise max regret computations, solving the above linear program for every different pair $(x,y) \in A^{2}$.
The max regret $\MR(x; \pprofile, W)$ is determined by considering the pairwise regret of $x$ with each other alternative in $A$.
The optimal alternative w.r.t. minimax regret is the one with least max regret. 
Notice that, whenever the PMR of an alternative $x$ (against some other alternative) exceeds the best MR value found so far, we don't need to further evaluate $x$. 
This idea can be exploited further by adopting a minimax-search tree (see \cite{Braziunas2011}).
%The computation can be made faster by adopting an efficient scheme of evaluation of PMR where .

\section{Interactive Elicitation} 
\label{sec:elicit}
We propose an incremental elicitation method based on minimax regret.
At each step, the system may either ask a question to one of the agents, or ask a question to the chair about the voting rule. 
The goal is to acquire relevant information to reduce minimax regret as quickly as possible.
%Minimax regret can be used to guide the elicitation process.
As termination condition of elicitation, we can check whether minimax regret is lower than a threshold; if we wish optimality, we can perform elicitation until minimax regret drops to zero.

The remainder of this Section is structured as follows.
First of all, we discuss the different types of questions that can be asked to the agents and to the chair, and the way responses are handled.
Then, we describe different strategies to determine informative queries to ask next, with the goal of reducing $\MMR(\pprofile,W)$ quickly.

%Type of questions that we can ask to the chair:	bound queries (Is this alternative among your top-k most preferred items?), comparison queries (do you prefer alternative x or alternative y?).
% we discuss the different forms of queries and develop several query strategies 
\paragraph{Question types}
We distinguish between questions asked to the agents and questions asked to the chair.
As {\em questions asked to the agents} it is natural to consider comparison queries asking to compare two alternatives.
%Another common type of queries are {\em top-k}, asking to each voter her $k$ most preferred alternatives.
The effect of a response to a question asked to an agent is to increase our knowledge about the agents rankings, thus augmenting the partial profile $\pprofile$. 
If voter $j$ answers a comparison query stating that alternative $a$ is preferred to $b$, then the partial order $\ppref_j$ is augmented with $a \ppref_j b$ and by transitive closure.

A bit more discussion is needed about {\em questions asked to the chair}.
Such questions aim at refining our knowledge about the scoring rule; a response gives us a constraint on the weight vector $\w$.
In particular, we want to acquire constraints of the type:
\[ w_{r} - w_{r+1} \geq \lambda (w_{r+1} - w_{r+2}) \] 
for $r \in \{1,\ldots,m-2\}$, relating the difference between the importance of ranks $r$ and $r+1$ with the difference between ranks $r+1$ and $r+2$.

\paragraph{Building concrete questions for the chair}
As we assume the weights constitute the utility components of the chair, it might be reasonable to assume, 
%as we do here, 
that the chair is able to answer such abstract questions in our setting. However, it is important to make sure that the question can also, in principle, be asked in a more concrete way, in terms of winners of example profiles. This permits to test whether the chair understands the question as we do; this permits to relate the preference of the chair  to her choice behavior in the economic sense; and this will be necessary for an ordinal extension of our work where the scores would not be considered as cardinal utilities.
Thus, our task is to build a profile, given $\lambda$ and $r ≤ m-2$, in such a way that the set of (tied) winners picked by the chair reveals whether $w_{r} - w_{r+1} \geq \lambda (w_{r+1} - w_{r+2})$.
We assume that $\lambda$ is rational, and define $\lambda = p/q$.

Observe that the question may be defined equivalently as whether $q w_{r} + p w_{r+2} ≥ (p + q) w_{r+1}$, where $p, q$ are natural numbers. 
As a first attempt to make this question concrete, let us define a profile $P$ containing $p+q$ agents. 
Define an alternative $a$ that receives $q$ times the rank $r$ and $p$ times the rank $r+2$ and an alternative $b$ that receives $p+q$ times the rank $r+1$. 
Observing that the score of $a$ in that profile is exactly the left hand side of the question, and that the score of $b$ is the right hand side, intuition suggests that observing whether the chair picks $a$ or $b$ as winner will let us determine which side is greater; equality occurring when the chair declares $a$ and $b$ as tied for the victory. 
However, we still need to complete the profile, thus, come up with other $m-2$ alternatives defined so that each agent in the resulting profile has placed exactly one alternative in each rank. 
And we must ensure, doing this, that these other alternatives are not better than $a$ or $b$.\footnote{Because, if the chair picks a different alternative $c$, it tells us that the chair prefers $c$ to both $a$ and $b$, 
%but it generally does not  answer the question we are interested in
but in general we do not acquire the type of constraint we are interested in.} 
Taking $p = q = 1$, $m=4$, $r=2$, we see that such a completion may be impossible. Luckily, this problem can be worked around, at the price of increasing the number of agents. 

First build a temporary profile $P$ of $p+q$ agents with $a$ and $b$ ranked as just described. Complete it with $m-2$ alternatives ranked in arbitrary orders so that the resulting profile has complete strict rankings for each of the $p+q$ voter. Now we will make the other alternatives as bad as desired by adding agents to $P$ that appreciate $a$ and $b$ more than the other alternatives, thus building a profile $P'$ in which, whatever the weights, $c$ may not have a better score than $a$. Observe that if we add $\delta$ agents (with $\delta$ a natural number) that put $a$ at first rank and $b$ at second rank, and $\delta$ agents that put $b$ at first rank and $a$ at second rank, we do not change the difference of the scores of $a$ and $b$, and thus, observing the chair choosing $a$ or $b$ still answers the question. Choose arbitrarily a matching of the remaining $m-2$ alternatives to the ranks $3$ to $m$, say, $c \mapsto 3$, $d \mapsto 4$, and so on. The first $\delta$ supplementary agents, who rank $a$ first and $b$ second, rank the other alternatives as the just defined mapping suggests. The other $\delta$ supplementary agents, who rank $b$ first and $a$ second, use the opposite ranking for the remaining alternatives, say, $c \mapsto m$, $d \mapsto m-1$, and so on. We can now prove that, for $\delta = p+q$, and whatever the weight vector $w \in \W$, no alternative, except possibly $b$, has a better score than $a$. (In fact, as the construction will exhibit, they are all worst than both $a$ and $b$, but the weaker fact is enough for our claim.)

Pick any alternative $c$ that is not $a$ or $b$. 
%Let $c_t$ denote the number of times $c$ reaches rank $t$ or better in $P$. Note that $c_m = p+q$ and $c_t ≤ p + q, 1 ≤ t ≤ m$. Define $a_t$ similarly, for the alternative $a$. By construction, $a_r = q$, $a_{r+2} = q+p = a_m$. 
Define $c'_t$ as the number of times $c$ gets rank $t$ or better in $P'$, and define $a'_t$ similarly. Now $a'_1 ≥ \delta, a'_t ≥ 2\delta \text{ for } 2 ≤ t < m, a'_{r+2} = q+p+2\delta = a'_m$. To obtain upper bounds for $c'_t$, assume $c$ is ranked first by the $p+q$ agents voting in profile $P$. By our construction, the two times $\delta$ new agents of $P'$ give to $c$ the ranks $(3, m)$, or $(4, m-1)$, … Observe that the score of $c$ is maximal for attribution $(3, m)$, by convexity of the weights. Therefore, in the best case for $c$, $c'_1 ≤ p + q, c'_2 ≤ p+q, c'_t ≤ p+q+\delta \text{ for } 3 ≤ t < m, c'_m ≤ p+q+2\delta$. Observe that $a'_t ≥ c'_t, 1 ≤ t ≤ m$ (with strict inequality for $t=2$). Because weights are non increasing, this guarantees, with $\delta = p + q$, that the score of $a$ is not lower than the one of $c$ (it is even strictly higher, thanks to the strict inequality). This concludes the proof of the following claim.

\begin{claim}
	Given a rational $\lambda$ and a rank $r$ between $1$ and $m - 2$, the profile named $P'$ in the foregoing description is such that, for any weight vector $\w \in \W$, $a \in f(P')$ iff $w_{r} - w_{r+1} ≥ \lambda (w_{r+1} - w_{r+2})$ and $b \in f(P')$ iff $w_{r} - w_{r+1} ≤ \lambda (w_{r+1} - w_{r+2})$, where $f$ is the PSR parameterized with $\w$.
\end{claim}

%\commentOC{I suppose we have, unfortunately, to get rid of this useful example, for reason of space. (If we manage to keep it, I’ll need to rework it a bit so that it follows the strategy in the proof.)}
\begin{example}
Suppose our query strategy suggests us to ask the chair the following query: $w_{2} - w_{3} \geq 2(w_{3} - w_{4})$.
In order to impose this constraints,
It is straightforward to see that we want the  chair of the committee to choose between $a$ or $b$ with the following rank distribution:
\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a & 0& 1& 0& 2 \\
b& 0& 0& 3& 0 \\
\end{array}
\]
In order to obtain a valid profile, we need to include a number of other alternatives in such a way that the are dominated by $a$ and $b$.
The following profile is exactly what we want,
%We add agents in order to increase the number of times $a$ and $b$ are ranked at first position. Please note we must maintain these scheme, so every add of a position to the ranking of $a$ must correspond to the same in the ranking of $b$.

\[
\begin{array}{cccccc}
v_1& v_2& v_3 & v_4& v_5& v_6 \\
\midrule 
a& b& c & d& a& b \\
c& a& d& c& b& a \\
b& d& b& b& d& c \\
d& c& a& a& c& d \\
\end{array}
\]
giving the following rank distribution (number of times each alternatives is ranked first, second, etc): 
\[
\begin{array}{ccccc}
& 1^\circ& 2^\circ& 3^\circ& 4^\circ \\
\cmidrule{2-5}
a & 2& 2& 0& 2 \\
b& 2& 1& 3& 0 \\
c& 1& 2& 1& 2 \\
d& 1& 1& 2& 2 \\
\end{array}
\]
Indeed alternative $c$ and $d$ are dominated and they cannot win for any instantiation of the weights; this can be seen by comparing the cumulative ranks (see  \citep{Stein1994}).
\[
\begin{array}{ccccc}
& \geq 1^\circ
& \geq 2^\circ
& \geq 3^\circ
& \geq 4^\circ \\
\cmidrule{2-5}
a & 2 & 4& 4& 6 \\
b & 2 & 3& 6& 6 \\
c& 1& 3& 4& 6 \\
d& 1& 2& 4& 6 \\
\end{array}
\]
\end{example}



\paragraph{Elicitation strategies}

We develop some elicitation strategies for simultaneous elicitation of agents' preferences and of the scoring rule.
Starting from some initial partial knowledge, our goal is to acquire the relevant preference information 
%(about both the scoring rule function and the agents' preferences) 
in order to determine an approximate winner.
While it is of course possible doing full elicitation of the weights and afterwards elicit the agents' preferences (or the other way around) we propose an interleaved approach.
In our interactive  protocol for simultaneously eliciting the preferences of the chair about the voting rule and the agents' preferences about the alternatives.
Indeed, it can be beneficial to interleave questions asked to the chair and questions asked to agents, depending on which is estimated to be more informative.

Answers given by the chair about the scoring rule refine our knowledge of the weights $w_1,\ldots,w_n$, while
answers given by one of the agents refine our knowledge about the agents preferences.
At each step we need to decide whether we want to ask a question to the chair or to one of the agents (and to which agent in particular). 
%We ask comparison queries to the agents and questions comparing the differences of weights to the chair. 

In the experiments we want to compare the interleaved approach with a baseline challenger, a method  that elicits the preferences of the agents first and then the voting rule (or the other way around)
We consider different strategies to determine the next question to ask given the current information.

%\begin{itemize}
%\item
\medskip \noindent
{\em Current solution strategy}: consider the solution of the minimax regret game, 
 $(x^{*},\bar{y},\bar{\profile},\bar{\w})$, where $x^{*}$ is the minimax regret optimal alternative,  $\bar{y} = \arg \max_{y \in A} \PMR(x^{*},y; \pprofile, W)$; is the adversarial choice, and $\bar{\profile}$ and $\w$ are the instantiations\footnote{ $\bar{\profile} \!=\! \arg \max_{\profile \in C(\pprofile)} s(y; \profile, \bar{\w}) \!-\! s(x; \profile, \bar{\w})$; and $\bar{\w} \!=\! \arg \max_{w \in W} s(y; \pprofile, w) \!-\! s(x; \pprofile, w)$ .} of the profile and weights associated with maximum regret.
 % Determine the part of ``regret'' due to uncertainty in $w$
% Determine the part of ``regret' due to uncertainty in $p$
Define: 
\begin{align}
\tau_{1} & = \min_{\w \in W} s(\bar{y}; \bar{\profile}, \w) - s(x^{*}; \bar{\profile}, \w)  \label{eq:tau1} \\
\tau_{2} & = \min_{\profile \in C(\pprofile)} s(\bar{y},\profile, \bar{\w}) -  s(x^{*}; \profile, \bar{\w}).
\end{align}
Intuitively the quantity $\MMR - \tau_{1} $ measures the contribution to regret of our uncertainty about the weights $\w$, while $\MMR - \tau_{2} $ measures the uncertainty about the profile.
Therefore, whenever $\tau_{1} < \tau_{2}$ we ask a question to the chair, and compare $\bar{\w}$ and $\w_{\tau}$. the argmax of Eq. (\ref{eq:tau1}), componentwise to find the position $r$ that is most valuable to ask about.  
Instead, if $\tau_{1} > \tau_{2}$ we ask a question to one of the agent, with the following heuristics: COMPLETE
 
 %\item 
 One possibility is to consider the MMR {\em a posteriori}. Assume that the different answers to a query induce the possible sets to be $(\pprofile_1,W_1)$ and $(\pprofile_2,W_2)$, then the score according to worst-case maximum regret is:
\[\SCORE(x)= \max_{i=1,2} \MMR(\pprofile_i,W_i) \]
In this case, the query with least value is chosen.
What is the complexity of this strategy? Let's define $Q$ as the set of all possible queries we can address to the agents and to the chair. The worst case is represented by the situation of complete ignorance.
%in which we do not have any information about both the agents' preferences and the weights constraints. 
In this case we can address $\binom{m}{2}=\frac{m(m-1)}{2}$ questions to each agent. Moreover, we need to ask at least $m$ questions to the chair. Then, each question induces two possible pairs of $(\pprofile,W)$ for each of which we compute the $\MMR$, whose worst case complexity is $O(nm^3)$. Thus, the cardinality of $Q$ in the worst case is $\approx n \cdot \frac{m(m-1)}{2} + \lambda m$, and the worst case time complexity of the strategy is $O(n^2m^5)$.

\medskip \noindent
{\em Two phase method.}
Ask a predefined (non adaptive) sequence of questions in order to learn the weights $w$ of the scoring rule).
Then use minimax regret as in Lu and Boutilier.

%\item Non-interleaved regret method: comparison with a method based on minimax regret, but not interleaved. 
\medskip \noindent
{\em Non-interleaved non-regret strategies (useful as benchmarks)}	
	\begin{itemize}
		\item Random strategy: randomly chooses an agent $i$ and a comparison query such that $x \pinc_i y$.
		\item {\em Volumetric} strategy: chooses an agent $i$ and a query that maximizes the number of new pairwise preferences revealed given the worst response.
	\end{itemize}
%\end{itemize}

% unless $\MMR=0$ we ask a question that reduce uncertainty

\section{Empirical Evaluation} \label{sec:experiments}

{\em {\bf TODO:} update with actual experiments}

\begin{figure}[t]
\begin{center}PLACEHOLDER\end{center}
\vspace{3.5cm}
\caption{Minimax regret reduction in our experiments (synthetic dataset).}
\end{figure}

\begin{figure}[t]
\begin{center}PLACEHOLDER\end{center}
\vspace{3.5cm}
\caption{Minimax regret reduction in our experiments (real dataset).}
\end{figure}

% We can stop elicitation when minimax regret is sufficiently small.
We test the elicitation protocols in randomly generated datasets and real datasets. % (sushi and iris).
We evaluate the proposed method using the following simulated protocol.
Our goal is to 
 to verify our intuition that an interleaved elicitation approach (mixing questions to the chair and questions asked to the agents) is more efficient than performing the elicitation of the rule first and than eliciting the agents preferences.
First of all, we randomly generate the true preferences of the users (i.e. the linear orders) and the weights associated with the chair's scoring rule.
For each of the elicitation strategy, we simulate an elicitation session.
%Queries are posed to simulated users, that answer accor
%We compare the performance  of the different strategies of Section \ref{sec:elicit} with respect to decrement in max regret and with respect to the real loss.
We measure the effectiveness of our strategies of Section \ref{sec:elicit} by examining regret reduction as a function of the number of queries.

%For each of the elicitation strategy, we simulate the elicitation by asking the queries selected by the strategy.
We performed tests with different values of $m$ (number of alternatives) and $n$ (number of agents); test with different population sizes, different number of alternatives, etc.

We experimented with both settings that assume a convex sequences of weights, and others without this assumption.

{\em {\bf TODO:} say something about computation times}

%Comparison between our interleaved strategy and a strategy, still based on minimax regret  but that is not interleaved; also compare to some heuristic baselines.

\section{Conclusions}  \label{sec:conclusions}
In this paper we have considered a  social choice setting with partial  information about the voter's  
preferences and as well a partially specified voting rule.
We have proposed the use of minimax regret as a means of robust winner determination in this setting, and as well as to guide the process of simultaneous elicitation of preferences and voting rule.
Our experimental results %on randomly generated and real world data sets 
show that regret-based elicitation is effective and allow to quickly find a near-optimal consensus choice.
%Regret-based elicitation allows to determine near-optimal winners using only few information about the agents' preferences.

We mention some  important directions for future works.
First of all, further development of elicitation strategies, considering alternative heuristics, is an important direction. 
Second, we are interested in extending elicitation of voting rules going beyond scoring rules.
A third direction is that of considering probabilistic methods for elicitation.
Finally, we are interested in studying the effect of strategic agents. %that may not report their true preferences.
% distribution-free heuristics

% Acknowledgements: We thank the reviewers for comments helping to improve the paper. 
%{\small
\bibliography{biblio}
\bibliographystyle{named}
%\bibliographystyle{plain} 
%}

\pagebreak
\ifappendix
\appendix
\section{Minimax Computation under Convex Assumption} 

Refer to \citep{Lu2011}
The goal is to choose as a winner the alternative $x^*$ whose worst case loss is minimal under all possible realizations of the full profile and all possible choices of weights. 
Assume hereinafter the selected weights sequence $\w \in W$ to be convex. 
In order to compute the minimal max regret $\MMR(\pprofile)$ under partial profile $\pprofile$ we need to compute the pairwise max regret between all pairs of alternatives $(x,y)$, where $x$ is a proposed winner and $y$ is the ``adversary'' alternative. Indeed, the construction of $\PMR(x,y,\pprofile,\w)$ can be viewed as an adversary's attempt to maximize the regret of choosing $x$ instead of $y$. 
For doing this, he can choose a completion $\profile_i \in C(\pprofile_i)$ of the partial profile and a (feasible) scoring vector $\w$ that maximize the contribution of the voter $i$ to $\PMR(x,y,\pprofile,\w)$. Let us now analyze how it could be done depending on the relation between alternatives $x$ and $y$ in $\pprofile_i$. 
\begin{itemize}
	\item $x \succ_i^\pprofile y$
	\newline If we know $x$ is preferred to $y$ and we choose $x$ as a winner, $\pprofile_i$ contribution to $\PMR(x,y,\pprofile,\w)$ must be negative. In this situation, our adversary can only try to minimize this advantage by minimizing the positional gap between the two alternatives. To achieve that, he can arbitrary place all the alternatives preferred to $x$ above $x$, together with all the ones with unknown relation to $x$. Moreover, he can place all the alternatives less preferred to $x$ and with unknown relation to $y$ below $y$. We can summarize it for each $q \in A$ as follows:
	\begin{align*}
	q \succ_i^\pprofile x \vee q \ ?_i^\pprofile \ x \ & \Rightarrow \ \uparrow_x \\
	x \succ_i^\pprofile q \wedge ( q \ ?_i^\pprofile \ y \vee y \succ_i^\pprofile q) \ & \Rightarrow \ \downarrow_y \\
	x \succ_i^\pprofile q \succ_i^\pprofile y \ & \Rightarrow \ \text{in between} \\
	\end{align*}
	It is worth noting that when the relation between $q$ and $x$ is not known in the partial profile, the adversary takes advantage by placing $q$ above $x$ only under the assumption of convex weight sequences.
	\item $y \succ_i^\pprofile x$
	\newline If $y$ is preferred to $x$ the construction proceeds similarly to the previous case, but now the adversary takes advantage by maximizing the gap between $x$ and $y$ placing as much alternatives as he can between the two. We can summarize the procedure for each $q \in A$ as follows:
	\begin{align*}
	q \succ_i^\pprofile y \ & \Rightarrow \ \uparrow_y \\
	x \succ_i^\pprofile q \ & \Rightarrow \ \downarrow_x \\
	(y \succ_i^\pprofile q \vee y \ ?_i^\pprofile \ q) \wedge (q \succ_i^\pprofile x \vee q \ ?_i^\pprofile \ x) \ & \Rightarrow \ \text{in between} \\
	\end{align*}
	\item $x \ ?_i^\pprofile \ y$
	\newline If the partial profile $\pprofile_i$ does not specify the relation between $x$ and $y$, the advantage is maximized by ordering $y$ over $x$ and maximizing the gap between them following the procedure for the case $y \succ_i^\pprofile x$.
\end{itemize}

\section{Dropping the Convex Assumption}
\subsection{Profile completion}
What if the sequence of weights is not convex? When $y \succ_i^\pprofile x$ or $x \ ?_i^\pprofile \ y$ weights do not influence the arbitrary placement of alternatives. Please remind we are working under the assumption that weights constitute a monotonic non-increasing sequence. Thus, there is no way for the adversary to take advantage from the weights distribution in order to increase the gap between $y$ and $x$ besides placing as much alternatives as he can between the two. The only case in which weights can influence the positional gap between $x$ and $y$ is when $x \succ_i^\pprofile y$ and $q \ ?_i^\pprofile \ x$. For convex sequences we place such alternatives $q$ above $x$, but it is not obvious that this is the best option for other sequences. For example, suppose $x$ and $y$ are ranked respectively in first and second position in the partial profile and we wonder where to place an alternative $q$ with unknown relation to $x$ (and thus to $y$). Suppose also that in the weight sequence the distance between the first and second positions is much lower than the one between the second and the third ones. In this case, placing $q$ above $x$ does not minimize the gap between $x$ and $y$ but we want, instead, to place $q$ below $y$.
\newline The constraints expressed by the chair may result in a set of feasible vectors such that none of them forms a convex sequence. In this case we need to analyze the particular sequence of weights in order to decide how to maximize the adversary advantage. Before going into details, let us define $A$ as the set of alternatives (if any) preferred to $x$, $B$ as the set of alternatives preferred to $y$ but not to $x$, and $U$ the set of those with unknown relation to both $x$ and $y$. The idea is to determine the positions that minimize $x$'s advantage over $y$ and then place some of the alternatives in $U$ above $x$ and some below $y$ in order to get that desired ranking. Since we cannot change the order of the alternatives in the set $B$ we know that $x$ and $y$ are separated exactly by $|B|$ positions (the adversary would not take any advantage by adding alternatives between them). So, starting from the position of $x$ in the partial completion of $\pprofile_i$ computed so far ($\hat{\profile}_i$), we find the two positions separated by $|B|$ alternatives whose weights difference is the lowest. Note that we can only add $|U|$ alternatives so we can check only until the position $\hat{\profile}_i(x)+|U|$. Algorithm \ref{alg:splittingU} shows the procedure described.

It is easy to see that we check at most $|U|$ positions. In the worst case the size of $U$ is equal to $m-2$, thus the procedure can be computed in $O(m)$ time. This cost does not affect the minimax regret computation time complexity that remains $O(nm^3)$.

\begin{algorithm}[h] 
	\caption{Placing alternatives in $U$ without Convex Assumption}
	\label{alg:splittingU} 
	\begin{algorithmic}
		\Require $x$, $y$, $\hat{\profile}_i$, $\w$, $U$, $B$
		\Ensure $\profile_i \in C(\pprofile)$
		\Statex
		\State $ j \gets 0$;
		\State $ i \gets \hat{\profile}_i(x)$;
		\State $ \mathit{posmin} \gets i$;
		\State $ \mathit{min} \gets \w(i) - \w(i+1+|B|)$;
		\While {$( j \leq |U| )$}
		\If{ $(\w(i+j)-\w(i+1+|B|+j) < \mathit{min})$ }
		\State $ \mathit{min} \gets \w(i+j) - \w(i+1+|B|+j)$;
		\State $ \mathit{posmin} \gets i+j$;
		\EndIf
		\EndWhile
		
		\State $U_{\mathit{abovex}} \gets (i-\mathit{posmin}) \text{ alternatives} \in U $;
		\State $U_{\mathit{belowy}} \gets U \setminus U_{\mathit{above}}$;
		\Statex
		\State $\profile_i \gets place(\pprofile_i,U_{\mathit{abovex}},U_{\mathit{belowy}})$;
		\Statex \Return $\profile_i$
		
	\end{algorithmic}
\end{algorithm}

%\subsection{Profile completion, again}
% %{\bf \em TODO: still have to find out exactly how to do it}
% 
% We introduce a set integer variables $I^i$, one for each position $i$.
% Let $U^j$ be, for the agents in $U^-$, the alternatives that, in the partial order $\succ^p_j$ of some agent $j \in U^-$, are incomparable with $x$ and $y$.
% \[ \sum_{l=1}^n I^j_l w_l \]
% Variable $I^j$ associated with the constraint $0 \leq I \leq |U^j|$.
% 
% We provide the following MIP (mixed integer linear program).
% 
%% Use linearization techniques to handle the multiplications...
%
%
%%\section{Even more fun}
%%We can consider different types of questions: asking to compare a pair of alternatives, asking about top-k alternatives.
%

\section{Minimax regret without the convex assumption}
Without the convex assumption, we cannot use $\hat{v}$ for the agents in $U^{-}$, but only for agents in $U^{+}$ and $U^{?}$.
%Let $\hat{v}_i$ be the linear order extending $\succ^{p}_i$ according to the above procedure.
Then PMR can be written as follows:
\begin{align*}
 &\PMR(x,y; \pprofile, W) =\\ 
 &\max_{\w \in W} \Big \{ \sum_{j \in U^-} [\max_{v_j \in C(\succ_j^p)} [w_{v_j(y)} \!-\! w_{v_j(x)}]] 
  \!+\!	 \sum_{j \in A^+ \cup  U^?} [w_{\hat{v}_j(y)} \!-\! w_{\hat{v}_j(x)}] \Big \} 
 \end{align*}
%Note that the second addendum inside the max do not depend on the choice of $\w$.
Consider the two addenda inside the ``max''. 
The first addendum is concerned with positioning of alternatives $x$ and $y$ for agents in $U^{-}$ for which we know that $x$ is preferred to $y$.
The second addendum is concerned with agents in $U^{+}$ and $U^{?}$.
We rewrite the second addendum as:
\begin{align*}
\sum_{j \in U^+ \cup  U^?} [w_{\hat{v}_j(y)} \!-\! w_{\hat{v}_j(x)}] 
= \sum_{i = 1}^{m} (\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i}
\end{align*}
where $\hat{\alpha}_{i}^{x}$ is the number of times that $x$ is ranked  in position $i$  considering the profile $\hat{v}$ of agents in $U^+ \cup  U^?$.
%We compute pairwise maximum regret by considering binary variables $\{ B_{i} \}_{i=1,\ldots,m}$ to represent optimization choices related to where to position the alternatives.

We now address the agents in $A^{-}$
For a given $j$, let $\beta$ be the number of alternatives that are incomparable with $x$ and $y$:
\[ \beta_{j} = \mid \{ c : c \pinc_{j} y \wedge c \pinc_{j} x \} \mid \]
$x$ can be ranked between $t_{1}(j)=\mid \{ c \in A : c \ppref_{j} y \wedge x \nppref_{j} c \} \mid $ and position $t_{2}(j)=t_{1}(j)+\beta_{j}$.
The completion for agent $j$ is such that the positions of $x$ and $y$ differ of exactly $\gamma_{j} =
\mid \{ c \in A : x \ppref c \ppref y \} \mid$ positions.


We now show how to optimize $\PMR$.
In addition to variables $\{ w_{j} \}_{j=1,\ldots,m}$ (one for each position) we need to employ several additional decision variables.
We introduce two sets of binary variables $B^{+}_{i,j}$ and $B^{-}_{i,j}$  for each position $i$ and for each voter $j$.
Variable $B_{i,j}^{+}$ encodes the fact that the alternative $y$ is placed in position $i$ in the ranking of agent $j$; while  $B_{i,j}^{-}$ encodes the same thing for alternative $x$.
%We also have numerical variables to represent the weights of the scoring rule.
Since each alternative needs to be placed exactly in one place for each agent, we adopt the constraints
$\sum_{i=t_{1}(j)}^{t_{2}(j)} B_{i,j}^{+} = 1$.
Since we know that $x$ and $y$ are ranked $\gamma_{j}$ positions apart, we set the constraint:
$B_{i+\gamma_{j},j}^{-} \geq B_{i,j}^{+}$,  for $i = \{ t_{1}(j), \ldots, t_{2}(j)\}$.

% and $\sum_{i=1}^{m} B_{i,j}^{-} = 1$.
%Since the objective is to maximize pairwise regret...

The score of alternative $y$ can be written as $\sum_{i = 1}^{m} \hat{\alpha}_{i}^{y}  w_{i} + \sum_{i=1}^{n} \sum_{j=1}^{m} w_{j} B_{i,j}^{+}$.
The objective function is now:
 \[ \max \sum_{i = 1}^{m} (\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i} +  \sum_{i=1}^{n} \sum_{j=1}^{m} (B_{i,j}^{+} - B_{i,j}^{-})  w_i \]

We use integer programming enconding tricks in order to linearize the problem.
We introduce yet another set of variables  $V_{i,j}^{+} $  and $V_{i,j}^{-}$ % the multiplicative terms by new variables.
and we enforce that $V_{i,j}^{+} = B^{+}_{i,j} w_i$ by setting constraints $V_{i,j}^{+} \leq B^{+}_{i,j}$ and $V_{i,j}^{+}  \leq w_i$.
We have similar constraints for enforcing $V_{i,j}^{-} = B^{-}_{i,j} w_i$.

We therefore obtain the following mixed integer linear program:
\newcommand{\C}{\mathcal{C}}
\begin{align}
\max & \sum_{i=1}^m  [(\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i}] +
  \sum_{j \in A^{-}} \sum_{i=t_{1}}^{t_{2}}  [V_{i,j}^{+} - V_{i,j}^{-}]
\end{align}
\begin{align}
\text{ s.t. } &  \text{Equation } (\ref{eq:monotone}) & \\
&  \C(\w) &  \\
& \sum_{i=t_{1}}^{t_{2}} B_{i,j}^{+} = 1 & \forall j \in A^{-} \\
& B_{i+\gamma_{j},j}^{-} \geq B_{i,j}^{+} & \forall i \in \{ t_{1}, \ldots t_{2}\}, \forall j \in A^{-} \\
& V_{i,j}^{+} \leq B_{i,j}^{+}  & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-} \\
& V_{i,j}^{+} \leq w_i & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-} \\
& V_{i,j}^{-} \geq w_{i} + B_{i,j}^{-} - 1 & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-}\\
& V_{i,j}^{-} \geq 0 & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-}
\end{align} 
%We write $w \in W$ as a shourtcut to represent the constraints that the weights are chosen to be in the feasible set.
There are (at most) $nm$ binary variables and $m(n+1)$ numerical variables.
The optimization program can be solved by any suitable MILP solver, although it is not suitable to large problem instances.

% DONT KNOW IF WE HAVE TO FORMALIZE THIS AS A CLAIM
%\begin{claim}
%The $\PMR$ is computed using the above optimization problem.
%\end{claim}

\section{Proof of completion}
\label{sec:prfCompl}
Given $w$ and ${\pref} \in C(\ppref)$, let $w[\pref]$ denote $w_{|\pref(y)|} - w_{|\pref(x)|}$. \commentOC{Changing the notation to slightly reduce the number of symbols, to be discussed.}

Given ${\pref} \in C(\ppref)$, define $\pref^1$ as $\pref$ except that the elements in $C = \{c \pinc x \land x \pref c\}$ come just above $x$ in $\pref^1$, respecting in $\pref^1$ their internal ordering in $\pref$. We show first that $\pref^1$ is an element of $C(\ppref)$; and second that $\forall w: w[\pref^1] \geq w[\pref]$. 

First, it is an element of $C(\ppref)$ as it does not contradict $\ppref$. To show this, recall first that $\pref$ does not contradict $\ppref$. Left to show is that $\forall c \in C, d \notin C: x \pref d \pref c \Rightarrow d \pinc c$ (thus any element $d$ whose relation with $c$ has changed from $\pref$ to $\pref^1$ were incomparable with $c$ in $\ppref$). Consider any such pair $c, d$. As $d \notin C$, $\lnot(d \pinc x)$ or $d \pref x$. From the hypothesis, $d \pref x$ is excluded. 
As $\pref$ extends $\ppref$, $\lnot(d \ppref x)$ and $\lnot (c \ppref d)$. Because $\lnot (d \pinc x)$, $x \ppref d$. Thus, $\lnot (d \ppref c)$, otherwise by transitivity $x \ppref c$. The conclusion now follows from $\lnot (c \ppref d)$ and $\lnot (d \ppref c)$.

Second, in $\pref^1$, compared to $\pref$, $x$ is $|C|$ positions lower, and $y$ is at most $|C|$ positions lower, depending on where $y$ was in $\pref$. Hence, by convexity of the weights, $w[\pref^1] \geq w[\pref]$.

Now define $\pref^2$ as $\pref^1$ except that the elements from $C' = \{c \pinc y \land \lnot (c \pref^1 x \pref^1 y) \land c \pref^1 y\}$ move just below $y$, respecting in $\pref^2$ their internal ordering in $\pref^1$. Observe that this move keeps property (1), that was satisfied by construction in $\pref^1$, intact, and satisfies property (2). The rest of the reasoning is similar to the previous move, as the situations are symmetric. (To be checked?) Hence, ${\pref^2} \in C(\pref)$ and it satisfies both properties.

Given any $x, y, w$, as $C(\pref)$ has an element maximizing $w[.]$ (by finiteness of the set), the above reasoning shows that some element in $C(\pref)$ satisfies both properties and maximizes $w[.]$. And any element in $C(\pref)$ satisfying both properties attribute the same positions to $x$ and $y$, thus, maximizes $w[.]$.

\section{Considerations on weights}
\label{sec:weights}
Let us consider a monotonic non-increasing sequence of weights: $w_{1} \geq w_{2} \geq \ldots \geq w_{m}$. Without loss of generality, we can assume $w_1=1$ and $w_m=0$.

\begin{claim}
	\label{clm:wsequence}
	If the weight sequence is convex then $w_{1} > w_{2}$.
	\[\forall i \in \{1,\ldots,m\} \;\; w_i - w_{i+1} \geq w_{i+1}-w_{i+2} \Rightarrow w_{1} > w_{2} \geq \ldots \geq w_{m}\] 
\end{claim}
\begin{proof}
	By contradiction let assume $w_{1} = w_{2}$ then 
	\begin{align*}
	w_{1} - w_{2} \geq w_{2} - w_{3} &\geq \dots \geq w_{m-1} - w_{m} \\
	1 - 1 \geq 1 - w_{3} &\geq \dots \geq w_{m-1} - 0 \\
	0 \geq 1 - w_{3} &\geq \dots \geq w_{m-1}
	\end{align*}
	At this point either $0\leq w_{3}<1$ or $w_{3}=1$. In the first case 
	\[0 \ngeq 1 - w_{3}\]
	This breaks the convexity assumption so it is impossible.
	In the second case, by definition there is a $w_{i} \neq 1$ where $2 < i \leq m$. So it would be 
	\[0 \ngeq 1 - w_{i}\]
	for some $i$. Again, the convexity is not satisfied.
\end{proof}

\begin{corollary}
	\label{cor:weq}
	If the weight sequence is convex and $w_{i} = w_{i+1}$ for some $i$, then $w_{j}=0 \ \forall \
	j=i, \dots m$.
\end{corollary}


\section{Querying the chair}
Suppose our query strategy suggests us to ask the chair the following query:
\[ w_{2} - w_{3} \geq 2(w_{3} - w_{4}) \]
Then we can transform it in:
\begin{align}
\label{eqn:juryquery}
w_{2} - w_{3} &\geq 2 \cdot w_{3} - 2 \cdot w_{4} \notag \\
w_{2} + 2 \cdot w_{4} &\geq 3 \cdot w_{3} 
\end{align}
and ask the committee if they would prefer as a winner an alternative ranked first one time and third three times rather than an alternative ranked second four times.

Another way to query the committee, a more concrete one, is to present them a profile representing the situation described by the query and then deduce its answer from the selected winner. Obviously we have to be sure to choose a profile where only the alternatives we are interested in could be pick as winners. Therefore, we need a systematic way to construct a profile that reflects the situation outlined by the query for two alternatives and the others are not better than them.


Assume we have a profile of $3$ agents ranking $4$ alternatives. We can represent it by columns where each of them represents the preference ordering of one voter.
\[
\begin{array}{ccc}
v_1
& v_2
& v_3 \\
\midrule 
c
& d
& c \\
a
& c
& d \\
b
& b
& b \\
d
& a
& a \\
\end{array}
\]

Assuming anonymity, we can also write the profile expressing for each alternative $i \in A$ the number of agents placing it at a given rank.

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 0
& 2 \\
b
& 0
& 0
& 3
& 0 \\
c
& 2
& 1
& 0
& 0 \\
d
& 0
& 1
& 1
& 1 \\
\end{array}
\]

Recalling the query (\ref{eqn:juryquery}) we are considering as an example, it is easy to see that in the current profile alternatives $a$ and $b$ represent the situation as described by the query. Therefore, if after proposing this profile to the committee the winning alternative turns out to be $a$ then we know that $w_{2} - w_{3} > 2(w_{3} - w_{4})$; if, instead, the winner is $b$ we know that $w_{2} - w_{3} < 2(w_{3} - w_{4})$ and if both alternatives are picked as winners then $w_{2} - w_{3} = 2(w_{3} - w_{4})$. 

Nevertheless, in this example it is clear that $c$ will always be preferred to other alternatives (see Claim \ref{clm:wsequence} in Section \ref{sec:weights}). To see it we can express for each alternative the number of agents placing it at a given rank or at a higher one.

\[
\begin{array}{ccccc}
& \geq 1^\circ
& \geq 2^\circ
& \geq 3^\circ
& \geq 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 1
& 3 \\
b
& 0
& 0
& 3
& 3 \\
c
& 2
& 3
& 3
& 3 \\
d
& 0
& 1
& 2
& 3 \\
\end{array}
\]


\begin{claim}
	Consider a set $A$ of $m$ alternatives and let $r_k(i)$ be the number of times the alternative $i$ obtains a rank $k$ or a higher one. Consider two alternatives $ i,j \in A$, if $\forall \ k=2, \dots,m \ r_k(i)\geq r_k(j)$ and $r_1(i) > r_1(j)$ then $i$ is always preferred to $j$ for any choice of weights.
\end{claim}

\begin{proof}
	For our assumptions we know the sequence of weights is monotonic non-increasing and convex. Moreover, for the Claim \ref{clm:wsequence} in Section \ref{sec:weights}, we know that $w_1 > w_2$. Therefore, even in the worst case where $r_k(i) = r_k(j) \ forall \ k=2, \dots,m$ the sum of weights for alternative $i$ cannot be lower than the one for $j$. It is worth noting that we cannot say anything when $r_1(i) = r_1(j)$. Indeed, as said in Corollary \ref{cor:weq}, all the weights for other position but the first one can be equal, thus the number of agents ranking the alternatives at a certain position does not matter anymore.	
\end{proof}

So the strategy for querying the committee is to use two alternatives $i$ and $j$ to represent the query and complete the profile such that the other alternatives are all dominated by them. An algorithmic approach is to start from the initial profile and then add as many agents as needed that rank $i$ and $j$ as their top choices and the other alternatives afterwards. As an example let consider again the query \ref*{eqn:juryquery}. We want the committee to choose between $a$ or $b$:

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 0
& 2 \\
b
& 0
& 0
& 3
& 0 \\
\end{array}
\]

We add agents in order to increase the number of times $a$ and $b$ are ranked at first position. Please note we must maintain these scheme, so every add of a position to the ranking of $a$ must correspond to the same in the ranking of $b$.

\[
\begin{array}{cccccc}
v_1
& v_2
& v_3 
& v_4
& v_5
& v_6 \\
\midrule 
a
& b
& c 
& d
& a
& b \\
c
& a
& d
& c
& b
& a \\
b
& d
& b
& b
& d
& c \\
d
& c
& a
& a
& c
& d \\
\end{array}
\]

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 2
& 2
& 0
& 2 \\
b
& 2
& 1
& 3
& 0 \\
c
& 1
& 2
& 1
& 2 \\
d
& 1
& 1
& 2
& 2 \\
\end{array}
\]

\[
\begin{array}{ccccc}
& \geq 1^\circ
& \geq 2^\circ
& \geq 3^\circ
& \geq 4^\circ \\
\cmidrule{2-5}
a 
& 2
& 4
& 4
& 6 \\
b
& 2
& 3
& 6
& 6 \\
c
& 1
& 3
& 4
& 6 \\
d
& 1
& 2
& 4
& 6 \\
\end{array}
\]
\textbf{Remarks:}
\begin{itemize}
	\item We are considering $\lambda \in \mathbb{N} \setminus \{0\}$ but we could be interested in a real number. \textit{Solution}: we can multiply both sides.
\end{itemize}
\fi
\end{document}  
