%\input{rfia} 
\documentclass[a4paper,twoside]{article}
\usepackage{rfia2000}
\usepackage[T1]{fontenc}
%\usepackage[english]{babel}
\usepackage{times}

%\author{ID: 2696}
\author{\begin{tabular}[t]{c@{\extracolsep{6em}}c@{\extracolsep{6em}}c}
	B. Napolitano${}^1$ & O. Cailloux${}^1$ & P. Viappiani${}^2$ \\
	\end{tabular}
{} \\
\\
${}^1$        LAMSADE, UMR 7243, CNRS and Universit\'e Paris Dauphine, PSL Research University, Paris, France   \\
${}^2$        LIP6, UMR 7606, CNRS and Sorbonne Universit\'e, Paris, France
{} \\
\\
%Mon adresse compl\`ete \\
\{firstname.lastname\}@dauphine.fr, paolo.viappiani@lip6.fr\\
%{\bf Domaine principal de recherche}: RFP ou IA\\
%{\bf Papier soumis dans le cadre de la journée commune}: OUI ou NON
}

\thispagestyle{plain}
\pagestyle{plain}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\usepackage{textcomp}
\newunicodechar{⇒}{\implies}
\newunicodechar{≠}{\ensuremath{\neq}}
\newunicodechar{≤}{\leq}
\newunicodechar{≥}{\geq}
%… Horizontal Ellipsis
\DeclareUnicodeCharacter{2026}{\ifmmode\dots\else\textellipsis\fi}
\newunicodechar{∧}{\land}
\newunicodechar{∨}{\lor}
\newunicodechar{∩}{\cap}
\newunicodechar{∪}{\cup}
%¬ Not Sign
\DeclareUnicodeCharacter{00AC}{\ifmmode\lnot\else\textlnot\fi}
\newunicodechar{⇔}{\Leftrightarrow}
\newcommand{\N}{ℕ}
\newunicodechar{ℕ}{\mathbb{N}}
\usepackage{algorithm, algpseudocode}
\usepackage{amsmath,amssymb,enumerate,amsthm}
\usepackage{natbib}
\usepackage[strict]{siunitx}
\usepackage{hyperref}
\usepackage{mathrsfs}

\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}

\usepackage{bm}
\usepackage{empheq}
\usepackage{cleveref}
\usepackage{xcolor}

\newcommand{\email}[1]{\href{mailto:#1}{#1}}
\newcommand{\denselist}{\itemsep -2pt\topsep-6pt\partopsep-6pt}
\newcommand{\commentOC}[1]{\textcolor{blue}{\small$\big[$OC: #1$\big]$}}

\newcommand{\pref}{\succ}%real, connected pref, strict
\newcommand{\prefr}{{\succ}^\text{r}}%real, connected pref, strict
\newcommand{\ppreflarge}{\succeq^\text{p}}%partial pref
\newcommand{\ppref}{\succ^\text{p}}%partial pref
\newcommand{\pprefinv}{\prec^\text{p}}%partial pref
\newcommand{\nppref}{\nsucc^\text{p}}%negated partial pref
\newcommand{\linors}{\mathcal{L}(A)}
%https://tex.stackexchange.com/a/45732, works within both \set and \set*, same spacing than \mid (https://tex.stackexchange.com/a/52905).
\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}

%Thanks to https://tex.stackexchange.com/q/154549
\makeatletter
\newcommand{\newrelation}[2]{% #1 = control sequence, #2 = replacement text
  \@ifdefinable{#1}{%
    \def#1{%
    \@ifnextchar_{\csname\string#1\endcsname}{\mathrel{#2}}%
    }%
    \@namedef{\string#1}##1##2{\mathrel{#2_{##2}}}%
  }%
}
\makeatother

\newrelation{\pinc}{\!\parallel\!}%partial pref, complement (incomparable)
%\newrelation{\pinc}{Q^\text{p}}%partial pref, complement (incomparable)

\newcommand{\profile}{\bm{v}}%(complete) profile
\newcommand{\pprofile}{{\bm{p}}}%partial profile
\newcommand{\w}{\bm{w}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Co}{\mathcal{C}}
\newcommand{\pw}{W}%our knowledge about the weights
\newcommand{\powersetz}[1]{\mathscr{P}^*(#1)}
\newcommand{\strat}[1]{\emph{#1}}

\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\SCORE}{Score}
\DeclareMathOperator{\PMR}{PMR}
\DeclareMathOperator{\MR}{MR}
\DeclareMathOperator{\MMR}{MMR}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{claim}{Claim}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\DeclarePairedDelimiter\set{\{}{\}}
\DeclarePairedDelimiter\card{\lvert}{\rvert}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

%	\tolerance=2000
%	%Accept overfull hbox up to...
%	\hfuzz=1cm
%	%Reduces verbosity about the bad line breaks.
%	\hbadness 10000
%	%Reduces verbosity about the underful vboxes.
%	\vbadness=10000

\title{\Large\bf Simultaneous Elicitation of Committee and Voters Preferences}

\begin{document}
\date{}
\maketitle
\thispagestyle{empty}
%\subsection*{R\'esum\'e}
%{\em
%	Le choix social traite du probl\`eme de la d\'etermination d'un choix consensuel \`a partir des pr\'ef\'erences de diff\'erents agents. Dans le cadre classique, la r\`egle de vote est fix\'ee \`a l’avance et les informations compl\`etes concernant les pr\'ef\'erences des agents sont fournies. Dans cet article, nous supposons que la r\`egle de vote et les pr\'ef\'erences des agents sont partiellement connues. Ainsi, nous fournissons un protocole interactif d’\'elicitation bas\'e sur le regret minimax et d\'eveloppons plusieurs strat\'egies d’interrogation, associant questions au comit\'e et questions aux agents, afin d’obtenir les informations les plus pertinentes et de converger rapidement vers un r\'esultat optimal ou s'approchant de l'optimal.
%}
%\subsection*{Mots Clef}
%Choix social computationnel, IA dans l'incertain, \'elicitation des pr\'ef\'erences, regret minimax.

\subsection*{Abstract}
{\em
	Social choice deals with the problem of determining a consensus choice from the preferences of different agents. In the classical setting, the voting rule is fixed beforehand and full information concerning the preferences of the agents is provided.
	Recently, the assumption of full preference information has been questioned by a number of researchers and several methods for eliciting preferences have been proposed.
%	\commentOC{This “we go one step further” sounds odd here. I suggest one more sentence about the normal elicitation setting with only the agents preferences partially specified.}
	In this paper we go one step further and we assume that both the voting rule and the agents preferences are partially specified. In this setting, we present an interactive elicitation protocol based on minimax regret and develop several query strategies that interleave questions to the chair and questions to the agents in order to attempt to acquire the most relevant information in order to quickly converge to optimal or a near-optimal alternative.
}
\subsection*{Keywords}
Computational Social Choice, uncertainty in AI, preference elicitation, minimax regret.

\section{Introduction}
%Aggregation of preference information is a central task in many computer systems (recommender systems, search engines, etc) whose final goal is to find a consensus choice.
%In this contexts, a natural approach consists in looking at methods from social choice and see how they can be adapted for group decision-making in a computerized setting.
In a traditional social choice setting, both the social choice function and the full preference orderings of the agents are expressed beforehand. Nevertheless, requiring agents to express full preference orderings can be prohibitively costly (in terms of cognitive and communication cost), especially for decisions with large sets of alternatives.
This observation has motivated a number of recent works considering social choice with partial preference orders \citep{Xia2008, Pini2009, Konczak05} and incremental elicitation \citep{Kalech2011, Lu2011, Naamani-Dery2015} of agents’ preferences. 
Furthermore, several situations for which it may not be easy to precisely define the voting rule exist.
Indeed, it is possible that the chair has some preferences over the desired aggregation method, but is not able to formalize the voting rule upfront.
The work of Cailloux and Endriss \citep{Cailloux2014} provides elicitation methods for a quite general class of rules based on weak orders.
When considering positional scoring rules, several authors \citep{Stein1994, Llamazares2013, Viappiani2018} have worked on positional scoring rules with uncertain weights, assuming that the preferences of the agents are fully known.
In this paper we consider that both the agents’ preferences and the social choice rule are partially specified.
We develop methods for computing the minimax-optimal alternative using positional scoring rules and we provide incremental elicitation methods to acquire relevant preference information. We then discuss several heuristics that determine queries, either to an agent or to the committee, that quickly reduce minimax regret.
While previous works have considered either partial information about the agents preferences or a partially specified aggregation method, we do not know of any work considering both sources of uncertainty at the same time.

\section{Partial information}
\label{sec:background}
We consider a set $A$ of $m$ alternatives (products, restaurants, public projects, etc.) and a set $\set{1, …, n}$ of agents. Each agent $j$ comes from an infinite set $\N$ of potential agents and is associated to her “real” preference order $\pref_j \in \linors$ which is a linear order (connected, transitive, asymmetric relation) over the alternatives.
A {\em profile} is the association of a preference to each agent and it is equivalently represented by $(\pref_1,\ldots,\pref_n)$ or by $\profile=(v_1,\ldots,v_n) \in V$ where $v_j(i) \in \set{1, \ldots, m}$ denotes the rank (position) of alternative $i$ in the preference order $\pref_j$.
%Given $V$ the set of possible preference profiles, a social choice function $f : V \rightarrow \powersetz{A}$ associates a profile with a set of winners; where $\powersetz{A}$ represents the set of subsets of $A$ except for the emptyset.
A social choice function associates a profile with a set of winners; we consider {\em positional scoring rules (PSR)}, which attach weights to positions according to the scoring vector $(w_1, \ldots, w_m)$.
An alternative obtains a score that depends on the rank obtained in each of the preference orders and the winners are the alternatives with highest scores.
In this work we assume fixed, but unknown, a profile $\profile^* = (\pref^*_j, j \in \{1, \ldots, n\})$ and a weight vector $\w^*$.
Our knowledge at a given time of the preference of agent $j$ is encoded by a partial order over the alternatives, thus a transitive and asymmetric binary relation, denoted by $\ppref_j$; we assume that preference information is truthful, i.e. 
%${\ppref_j} \subseteq {\pref_j^*}$.
$a \ppref_j b ⇒ a \pref_j^* b$.
%\commentOC{Or simply ${\ppref_j} \subseteq {\pref_j^*}$, if you need to save space}
%We use $\pinc$ to denote incomparability, that is $a \pinc_{j} b$ iff $a \nppref_j b \wedge b \nppref_j a$.
An incomplete profile $\pprofile = (\ppref_1, \ldots, \ppref_n)$ maps each agent to a partial preference. A completion of $\ppref_j$ is any linear order $\pref$ that extends $\ppref_j$ and we indicate with $C(\ppref_j) = \set{{\succ} \in \linors \suchthat {\ppref_j} \subseteq {\succ}}$ the set of possible completions of $\ppref_i$.
%We let $C(\pprofile)=C(\ppref_1)\times … \times C(\ppref_n)$ be the set of complete profiles extending $p$. Note that $\profile^* \in C(\pprofile)$.

Our knowledge regarding the weights of the scoring rule is represented by
%We also assume that the weights of the scoring rule are only partially specified.
%Therefore the vector $(w_1,\ldots,w_m)$  is not known but we are given 
a set of constraints restraining the possible values that they can take. We consider a convex sequence of weights and we assume that 
%, i.e. a decreasing sequence $1=w_{1} > w_{2} > \ldots > w_{m}=0$ such that the difference between the weights of the first and second positions is at least as great as the difference between the weights of the second and third positions, etc.
%\begin{equation} 
%\label{eq:convexity}
%\forall r \in \{1,\ldots,m-2\}: w_r - w_{r+1} \geq w_{r+1}-w_{r+2}.
%\end{equation}
%We assume that in addition of basic requirements (monotonicity and convexity), 
the chair is able to specify additional preferences about how the social choice function should behave. 
%Of course it may be difficult for real decision makers to state these preferences, thus we can elicit them, for instance, by showing a complete profile of a small synthetic election and asking who should be elected in this case. \commentOC{Not entirely true as stated here. This works only in a subset of the case. So this remark should be dropped or amended, to say that we worked on this problem and it is still a work in progress to come up with an elicitation procedure that uses only questions that can effectively be asked using such a small synthetic election, and that it is not discussed further in this short article for lack of space. Also I’d remove “Of course”, it’s not obviously true.}
Such requirements are encoded with linear constraints about the vector $\w$ and the set of these constraints is denoted by $\Co_W$. Given the set of convex weight vectors $\W$, we use $\pw \subseteq \W$ to denote the set of weight vectors compatible with the chair's preferences.

\section[Minimax regret under partial profile and weight information]{
Robust winner determination}
\label{sec:mmr}
As a decision criterion to determine a winner, we propose to use minimax regret \citep{Savage1954}. 
%Minimax regret \citep{Savage1954} is a decision criterion that has been used for robust optimization under data uncertainty \citep{Kouvelis1997} as well as in decision-making with uncertain utility values \citep{Salo2001,Boutilier2006}.
%\citet{Lu2011} have adopted minimax regret for winner determination in social choice with
%the preferences of the agents that are only partially known, while the social choice function is predetermined and known.
%In this work we consider the simultaneous presence of uncertainty in the agents' preferences and uncertainty in the weights.
We use {\em maximum regret} to quantify the worst-case error, which measures, intuitively, how far an alternative is from the optimal one given current knowledge; 
% about the voting rule and the agents' preferences.
the alternatives that minimize this error are selected as tied winners.
%, providing us with a form of robust optimization.
The maximum regret of an alternative is the highest possible difference between its score and the one achievable by any alternative in any state compatible with our current knowledge.
%The maximum regret, given our current knowledge and an alternative $x$, is the highest possible difference between the score of $x$ and the score achievable by any alternative in any state compatible with our current knowledge.
It is considered by assuming that an adversary can both 1) extend the partial profile $\pprofile$ into a complete one, and 2) instantiate the weights choosing among any weight vector in $\pw$.
We formalize the notion of minimax regret in multiple steps.
First of all, $\Regret(x, \profile, \w)$ is the “regret” of selecting $x$ as a winner instead of choosing the optimal alternative under $\profile$ and $\w$:
\[\Regret(x, \profile, \w) = \max_{y \in A} s(y; \profile,\w) - s(x; \profile, \w).\]
The pairwise max regret $\PMR(x,y;\pprofile,W)$ of $x$ relative to $y$ given partial profile $\pprofile$ and the set of weights $W$ is the worst-case loss of choosing $x$ instead of $y$ under all possible realizations of the full profile {\em and} weights. Max regret $\MR(x;\pprofile,W)$ is the worst-case loss of $x$ and $\MMR(\pprofile,W)$ is the value of minimax regret obtained when recommending a minimax optimal alternative $x^*_{\pprofile, W}$. 
%\begin{align}
%\PMR(x,y; \pprofile, W) & = \max_{\w \in W} \max_{\profile \in C(\pprofile)} s(y; \profile,\w) - s(x; \profile,\w).
%\end{align}
%It is the loss occurred by an adversarial selection of a complete profile $\profile$ extending $\pprofile$ and a selection of $\w \in W$ to maximize the loss between $x$ and the true winner under $\profile$ and $\w$.
\begin{align}
\PMR(x,y; \pprofile, W) & = \max_{\w \in W} \max_{\profile \in C(\pprofile)} s(y; \profile,\w) - s(x; \profile,\w) \\
\MR(x; \pprofile, W) & = \max_{y \in A} \PMR(x,y; \pprofile, W)\\
%& = \max_{\w \in W} \max_{\profile \in C(\profile)} \Regret(x, \profile, \w) \\
\MMR(\pprofile,W) & = \min_{x \in A} \MR(x;\pprofile,W) \\
x^{*}_{\pprofile,W} \in A^*_{\pprofile, W} & = \argmin_{x \in A} \MR(x;\pprofile,W) 
\end{align}

%Finally,  $\MMR(\pprofile,W)$ is the value of minimax regret under $\pprofile$ and $W$, obtained when recommending a minimax optimal alternative $x^*_{\pprofile, W}$:
%\begin{align*}
%\MMR(\pprofile,W) & = \min_{x \in A} \MR(x;\pprofile,W) \\
%x^{*}_{\pprofile,W} \in A^*_{\pprofile, W} & = \argmin_{x \in A} \MR(x;\pprofile,W) 
%\end{align*}
By picking as consensus choice an alternative associated with minimax regret, we can provide a recommendation that gives worst-case guarantees, giving some robustness in face of uncertainty. 
$\PMR$ can be computed by adapting the reasoning of Lu and Boutilier \cite{Lu2011} to the case of uncertain weights using linear programming.
%In cases of ties in minimax regret, we can either decide to return all minimax alternatives $A^*_{\pprofile, W}$ as winners or to pick just one of them using some tie-breaking strategy.
%
%Notice that if $\MMR(\pprofile, W)\!=\!0$, then $x^{*}_{\pprofile,W}$ is a necessary co-winner; this means that for any valid completion of the profile and any feasible $w \!\in\! W$, $x^{*}_{\pprofile,W}$ obtains a highest score.
%
%We note that our notion of regret gives some cardinal meaning to the scores: instead of just being used to select winners under the corresponding PSR, their differences are considered as representing the regret of the chair.


\section{Interactive Elicitation} 
\label{sec:elicit}
We propose an incremental elicitation method based on minimax regret.
At each step, the system may ask a question either to one of the agents or to the chair.
The goal is to acquire relevant information to reduce minimax regret as quickly as possible.
As termination condition of elicitation, we can check whether minimax regret is lower than a threshold or, if we wish optimality, we can perform elicitation until minimax regret drops to zero.

\paragraph{Question types}
We distinguish between questions asked to the agents and questions asked to the chair.
For the first we consider comparison queries that ask a particular agent to compare two alternatives. Then, the partial profile $\pprofile$ is augmented on the basis of the agent's answer.
The latter, instead, aims at refining our knowledge about the scoring rule; in particular, we assume we can acquire constraints of the type:
\[ w_{r} - w_{r+1} \geq \lambda (w_{r+1} - w_{r+2}) \] 
for $r \in \{1,\ldots,m-2\}$, relating the difference between the importance of consecutive ranks.
% $r$ and $r+1$ with the difference between ranks $r+1$ and $r+2$.

\paragraph{Elicitation strategies}
A strategy is a function (or a random process) that, given our partial knowledge so far, gives us a question that should be asked. 

The \strat{Random} strategy first decides with a probability of $1/2$ each whether it will ask a question about weights or about a preference ordering. Then it equiprobably draws a question among the set of the possible ones. This strategy is clearly non efficient but it gives us a baseline for comparisons.

The \strat{Extreme completions} strategy considers the solution of the minimax regret game and estimates the contribution to the regret of our uncertainty about the weights and the profile. Then it asks a question to the chair or to the agents depending on which of these two values is the highest.

The \strat{Pessimistic} strategy selects among all the possible questions the one that leads to minimal regret in the worst case. For each question, it considers both possible answers to the question and uses some aggregation of their MMRs as a score of the question. 

The \strat{Two phase} strategy asks a predefined, non adaptive sequence of $m-2$ questions to the chair and then it only asks questions about the agents, using questions as defined in the \strat{Pessimistic} strategy.

\section{Conclusions}  
\label{sec:conclusions}
In this paper we have considered a social choice setting with partial information and we have proposed the use of minimax regret both as a means of robust winner determination as well as a guide to the process of simultaneous elicitation of preferences and voting rule.

The next step is to test our strategies in order to compare the performance of our interleaved elicitation approach that mixes questions to the chair and to the agents, to a more classical approach that elicits the rule first and then the agents preferences (or vice-versa). 

Further development of elicitation strategies, considering alternative heuristics, is also an important direction for future works together with the extension of the approach to voting rules beyond scoring rules.

% Acknowledgements: We thank the reviewers for comments helping to improve the paper. 
%{\small
\bibliography{biblio}
\bibliographystyle{plain}
%\bibliographystyle{plain} 
%}
\end{document}

\appendix
\section{Minimax Computation under Convex Assumption} 

Refer to \citep{Lu2011}
The goal is to choose as a winner the alternative $x^*$ whose worst case loss is minimal under all possible realizations of the full profile and all possible choices of weights. 
Assume hereinafter the selected weights sequence $\w \in W$ to be convex. 
In order to compute the minimal max regret $\MMR(\pprofile)$ under partial profile $\pprofile$ we need to compute the pairwise max regret between all pairs of alternatives $(x,y)$, where $x$ is a proposed winner and $y$ is the ``adversary'' alternative. Indeed, the construction of $\PMR(x,y,\pprofile,\w)$ can be viewed as an adversary's attempt to maximize the regret of choosing $x$ instead of $y$. 
For doing this, he can choose a completion $\profile_i \in C(\pprofile_i)$ of the partial profile and a (feasible) scoring vector $\w$ that maximize the contribution of the agent $i$ to $\PMR(x,y,\pprofile,\w)$. Let us now analyze how it could be done depending on the relation between alternatives $x$ and $y$ in $\pprofile_i$. 
\begin{itemize}
	\item $x \succ_i^\pprofile y$
	\newline If we know $x$ is preferred to $y$ and we choose $x$ as a winner, $\pprofile_i$ contribution to $\PMR(x,y,\pprofile,\w)$ must be negative. In this situation, our adversary can only try to minimize this advantage by minimizing the positional gap between the two alternatives. To achieve that, he can arbitrary place all the alternatives preferred to $x$ above $x$, together with all the ones with unknown relation to $x$. Moreover, he can place all the alternatives less preferred to $x$ and with unknown relation to $y$ below $y$. We can summarize it for each $q \in A$ as follows:
	\begin{align*}
	q \succ_i^\pprofile x \vee q \ ?_i^\pprofile \ x \ & \Rightarrow \ \uparrow_x \\
	x \succ_i^\pprofile q \wedge ( q \ ?_i^\pprofile \ y \vee y \succ_i^\pprofile q) \ & \Rightarrow \ \downarrow_y \\
	x \succ_i^\pprofile q \succ_i^\pprofile y \ & \Rightarrow \ \text{in between} \\
	\end{align*}
	It is worth noting that when the relation between $q$ and $x$ is not known in the partial profile, the adversary takes advantage by placing $q$ above $x$ only under the assumption of convex weight sequences.
	\item $y \succ_i^\pprofile x$
	\newline If $y$ is preferred to $x$ the construction proceeds similarly to the previous case, but now the adversary takes advantage by maximizing the gap between $x$ and $y$ placing as much alternatives as he can between the two. We can summarize the procedure for each $q \in A$ as follows:
	\begin{align*}
	q \succ_i^\pprofile y \ & \Rightarrow \ \uparrow_y \\
	x \succ_i^\pprofile q \ & \Rightarrow \ \downarrow_x \\
	(y \succ_i^\pprofile q \vee y \ ?_i^\pprofile \ q) \wedge (q \succ_i^\pprofile x \vee q \ ?_i^\pprofile \ x) \ & \Rightarrow \ \text{in between} \\
	\end{align*}
	\item $x \ ?_i^\pprofile \ y$
	\newline If the partial profile $\pprofile_i$ does not specify the relation between $x$ and $y$, the advantage is maximized by ordering $y$ over $x$ and maximizing the gap between them following the procedure for the case $y \succ_i^\pprofile x$.
\end{itemize}

\section{Dropping the Convex Assumption}
\subsection{Profile completion}
What if the sequence of weights is not convex? When $y \succ_i^\pprofile x$ or $x \ ?_i^\pprofile \ y$ weights do not influence the arbitrary placement of alternatives. Please remind we are working under the assumption that weights constitute a monotonic non-increasing sequence. Thus, there is no way for the adversary to take advantage from the weights distribution in order to increase the gap between $y$ and $x$ besides placing as much alternatives as he can between the two. The only case in which weights can influence the positional gap between $x$ and $y$ is when $x \succ_i^\pprofile y$ and $q \ ?_i^\pprofile \ x$. For convex sequences we place such alternatives $q$ above $x$, but it is not obvious that this is the best option for other sequences. For example, suppose $x$ and $y$ are ranked respectively in first and second position in the partial profile and we wonder where to place an alternative $q$ with unknown relation to $x$ (and thus to $y$). Suppose also that in the weight sequence the distance between the first and second positions is much lower than the one between the second and the third ones. In this case, placing $q$ above $x$ does not minimize the gap between $x$ and $y$ but we want, instead, to place $q$ below $y$.
\newline The constraints expressed by the chair may result in a set of feasible vectors such that none of them forms a convex sequence. In this case we need to analyze the particular sequence of weights in order to decide how to maximize the adversary advantage. Before going into details, let us define $A$ as the set of alternatives (if any) preferred to $x$, $B$ as the set of alternatives preferred to $y$ but not to $x$, and $U$ the set of those with unknown relation to both $x$ and $y$. The idea is to determine the positions that minimize $x$'s advantage over $y$ and then place some of the alternatives in $U$ above $x$ and some below $y$ in order to get that desired ranking. Since we cannot change the order of the alternatives in the set $B$ we know that $x$ and $y$ are separated exactly by $|B|$ positions (the adversary would not take any advantage by adding alternatives between them). So, starting from the position of $x$ in the partial completion of $\pprofile_i$ computed so far ($\hat{\profile}_i$), we find the two positions separated by $|B|$ alternatives whose weights difference is the lowest. Note that we can only add $|U|$ alternatives so we can check only until the position $\hat{\profile}_i(x)+|U|$. Algorithm \ref{alg:splittingU} shows the procedure described.

It is easy to see that we check at most $|U|$ positions. In the worst case the size of $U$ is equal to $m-2$, thus the procedure can be computed in $O(m)$ time. This cost does not affect the minimax regret computation time complexity that remains $O(nm^3)$.

\begin{algorithm}[h] 
	\caption{Placing alternatives in $U$ without Convex Assumption}
	\label{alg:splittingU} 
	\begin{algorithmic}
		\Require $x$, $y$, $\hat{\profile}_i$, $\w$, $U$, $B$
		\Ensure $\profile_i \in C(\pprofile)$
		\Statex
		\State $ j \gets 0$;
		\State $ i \gets \hat{\profile}_i(x)$;
		\State $ \mathit{posmin} \gets i$;
		\State $ \mathit{min} \gets \w(i) - \w(i+1+|B|)$;
		\While {$( j \leq |U| )$}
		\If{ $(\w(i+j)-\w(i+1+|B|+j) < \mathit{min})$ }
		\State $ \mathit{min} \gets \w(i+j) - \w(i+1+|B|+j)$;
		\State $ \mathit{posmin} \gets i+j$;
		\EndIf
		\EndWhile
		
		\State $U_{\mathit{abovex}} \gets (i-\mathit{posmin}) \text{ alternatives} \in U $;
		\State $U_{\mathit{belowy}} \gets U \setminus U_{\mathit{above}}$;
		\Statex
		\State $\profile_i \gets place(\pprofile_i,U_{\mathit{abovex}},U_{\mathit{belowy}})$;
		\Statex \Return $\profile_i$
		
	\end{algorithmic}
\end{algorithm}

\section{Minimax regret without the convex assumption}
Without the convex assumption, we cannot use $\hat{v}$ for the agents in $U^{-}$, but only for agents in $U^{+}$ and $U^{?}$.
%Let $\hat{v}_i$ be the linear order extending $\succ^{p}_i$ according to the above procedure.
Then PMR can be written as follows:
\begin{align*}
 &\PMR(x,y; \pprofile, W) =\\ 
 &\max_{\w \in W} \Big \{ \sum_{j \in U^-} [\max_{v_j \in C(\succ_j^p)} [w_{v_j(y)} \!-\! w_{v_j(x)}]] 
  \!+\!	 \sum_{j \in A^+ \cup  U^?} [w_{\hat{v}_j(y)} \!-\! w_{\hat{v}_j(x)}] \Big \} 
 \end{align*}
%Note that the second addendum inside the max do not depend on the choice of $\w$.
Consider the two addenda inside the ``max''. 
The first addendum is concerned with positioning of alternatives $x$ and $y$ for agents in $U^{-}$ for which we know that $x$ is preferred to $y$.
The second addendum is concerned with agents in $U^{+}$ and $U^{?}$.
We rewrite the second addendum as:
\begin{align*}
\sum_{j \in U^+ \cup  U^?} [w_{\hat{v}_j(y)} \!-\! w_{\hat{v}_j(x)}] 
= \sum_{i = 1}^{m} (\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i}
\end{align*}
where $\hat{\alpha}_{i}^{x}$ is the number of times that $x$ is ranked  in position $i$  considering the profile $\hat{v}$ of agents in $U^+ \cup  U^?$.
%We compute pairwise maximum regret by considering binary variables $\{ B_{i} \}_{i=1,\ldots,m}$ to represent optimization choices related to where to position the alternatives.

We now address the agents in $A^{-}$
For a given $j$, let $\beta$ be the number of alternatives that are incomparable with $x$ and $y$:
\[ \beta_{j} = \mid \{ c : c \pinc_{j} y \wedge c \pinc_{j} x \} \mid \]
$x$ can be ranked between $t_{1}(j)=\mid \{ c \in A : c \ppref_{j} y \wedge x \nppref_{j} c \} \mid $ and position $t_{2}(j)=t_{1}(j)+\beta_{j}$.
The completion for agent $j$ is such that the positions of $x$ and $y$ differ of exactly $\gamma_{j} =
\mid \{ c \in A : x \ppref c \ppref y \} \mid$ positions.


We now show how to optimize $\PMR$.
In addition to variables $\{ w_{j} \}_{j=1,\ldots,m}$ (one for each position) we need to employ several additional decision variables.
We introduce two sets of binary variables $B^{+}_{i,j}$ and $B^{-}_{i,j}$  for each position $i$ and for each agent $j$.
Variable $B_{i,j}^{+}$ encodes the fact that the alternative $y$ is placed in position $i$ in the ranking of agent $j$; while  $B_{i,j}^{-}$ encodes the same thing for alternative $x$.
%We also have numerical variables to represent the weights of the scoring rule.
Since each alternative needs to be placed exactly in one place for each agent, we adopt the constraints
$\sum_{i=t_{1}(j)}^{t_{2}(j)} B_{i,j}^{+} = 1$.
Since we know that $x$ and $y$ are ranked $\gamma_{j}$ positions apart, we set the constraint:
$B_{i+\gamma_{j},j}^{-} \geq B_{i,j}^{+}$,  for $i = \{ t_{1}(j), \ldots, t_{2}(j)\}$.

% and $\sum_{i=1}^{m} B_{i,j}^{-} = 1$.
%Since the objective is to maximize pairwise regret...

The score of alternative $y$ can be written as $\sum_{i = 1}^{m} \hat{\alpha}_{i}^{y}  w_{i} + \sum_{i=1}^{n} \sum_{j=1}^{m} w_{j} B_{i,j}^{+}$.
The objective function is now:
 \[ \max \sum_{i = 1}^{m} (\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i} +  \sum_{i=1}^{n} \sum_{j=1}^{m} (B_{i,j}^{+} - B_{i,j}^{-})  w_i \]

We use integer programming enconding tricks in order to linearize the problem.
We introduce yet another set of variables  $V_{i,j}^{+} $  and $V_{i,j}^{-}$ % the multiplicative terms by new variables.
and we enforce that $V_{i,j}^{+} = B^{+}_{i,j} w_i$ by setting constraints $V_{i,j}^{+} \leq B^{+}_{i,j}$ and $V_{i,j}^{+}  \leq w_i$.
We have similar constraints for enforcing $V_{i,j}^{-} = B^{-}_{i,j} w_i$.

We therefore obtain the following mixed integer linear program:
\newcommand{\C}{\mathcal{C}}
\begin{align}
\max & \sum_{i=1}^m  [(\hat{\alpha}_{i}^{y} - \hat{\alpha}_{i}^{x}) w_{i}] +
  \sum_{j \in A^{-}} \sum_{i=t_{1}}^{t_{2}}  [V_{i,j}^{+} - V_{i,j}^{-}]
\end{align}
\begin{align}
\text{ s.t. } &  \text{Equation } (\ref{eq:monotone}) & \\
&  \C(\w) &  \\
& \sum_{i=t_{1}}^{t_{2}} B_{i,j}^{+} = 1 & \forall j \in A^{-} \\
& B_{i+\gamma_{j},j}^{-} \geq B_{i,j}^{+} & \forall i \in \{ t_{1}, \ldots t_{2}\}, \forall j \in A^{-} \\
& V_{i,j}^{+} \leq B_{i,j}^{+}  & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-} \\
& V_{i,j}^{+} \leq w_i & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-} \\
& V_{i,j}^{-} \geq w_{i} + B_{i,j}^{-} - 1 & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-}\\
& V_{i,j}^{-} \geq 0 & \forall i \in \{ t_{1}, \ldots t_{2}\}, j \in A^{-}
\end{align} 
%We write $w \in W$ as a shourtcut to represent the constraints that the weights are chosen to be in the feasible set.
There are (at most) $nm$ binary variables and $m(n+1)$ numerical variables.
The optimization program can be solved by any suitable MILP solver, although it is not suitable to large problem instances.

% DONT KNOW IF WE HAVE TO FORMALIZE THIS AS A CLAIM
%\begin{claim}
%The $\PMR$ is computed using the above optimization problem.
%\end{claim}

\section{Considerations on weights}
\label{sec:weights}
Let us consider a monotonic non-increasing sequence of weights: $w_{1} \geq w_{2} \geq \ldots \geq w_{m}$. Without loss of generality, we can assume $w_1=1$ and $w_m=0$.

\begin{claim}
	\label{clm:wsequence}
	If the weight sequence is convex then $w_{1} > w_{2}$.
	\[\forall i \in \{1,\ldots,m\} \;\; w_i - w_{i+1} \geq w_{i+1}-w_{i+2} \Rightarrow w_{1} > w_{2} \geq \ldots \geq w_{m}\] 
\end{claim}
\begin{proof}
	By contradiction let assume $w_{1} = w_{2}$ then 
	\begin{align*}
	w_{1} - w_{2} \geq w_{2} - w_{3} &\geq \dots \geq w_{m-1} - w_{m} \\
	1 - 1 \geq 1 - w_{3} &\geq \dots \geq w_{m-1} - 0 \\
	0 \geq 1 - w_{3} &\geq \dots \geq w_{m-1}
	\end{align*}
	At this point either $0\leq w_{3}<1$ or $w_{3}=1$. In the first case 
	\[0 \ngeq 1 - w_{3}\]
	This breaks the convexity assumption so it is impossible.
	In the second case, by definition there is a $w_{i} \neq 1$ where $2 < i \leq m$. So it would be 
	\[0 \ngeq 1 - w_{i}\]
	for some $i$. Again, the convexity is not satisfied.
\end{proof}

\begin{corollary}
	\label{cor:weq}
	If the weight sequence is convex and $w_{i} = w_{i+1}$ for some $i$, then $w_{j}=0 \ \forall \
	j=i, \dots m$.
\end{corollary}


\section{Querying the chair}
Suppose our query strategy suggests us to ask the chair the following query:
\[ w_{2} - w_{3} \geq 2(w_{3} - w_{4}) \]
Then we can transform it in:
\begin{align}
\label{eqn:juryquery}
w_{2} - w_{3} &\geq 2 \cdot w_{3} - 2 \cdot w_{4} \notag \\
w_{2} + 2 \cdot w_{4} &\geq 3 \cdot w_{3} 
\end{align}
and ask the committee if they would prefer as a winner an alternative ranked first one time and third three times rather than an alternative ranked second four times.

Another way to query the committee, a more concrete one, is to present them a profile representing the situation described by the query and then deduce its answer from the selected winner. Obviously we have to be sure to choose a profile where only the alternatives we are interested in could be pick as winners. Therefore, we need a systematic way to construct a profile that reflects the situation outlined by the query for two alternatives and the others are not better than them.


Assume we have a profile of $3$ agents ranking $4$ alternatives. We can represent it by columns where each of them represents the preference ordering of one agent.
\[
\begin{array}{ccc}
v_1
& v_2
& v_3 \\
\midrule 
c
& d
& c \\
a
& c
& d \\
b
& b
& b \\
d
& a
& a \\
\end{array}
\]

Assuming anonymity, we can also write the profile expressing for each alternative $i \in A$ the number of agents placing it at a given rank.

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 0
& 2 \\
b
& 0
& 0
& 3
& 0 \\
c
& 2
& 1
& 0
& 0 \\
d
& 0
& 1
& 1
& 1 \\
\end{array}
\]

Recalling the query (\ref{eqn:juryquery}) we are considering as an example, it is easy to see that in the current profile alternatives $a$ and $b$ represent the situation as described by the query. Therefore, if after proposing this profile to the committee the winning alternative turns out to be $a$ then we know that $w_{2} - w_{3} > 2(w_{3} - w_{4})$; if, instead, the winner is $b$ we know that $w_{2} - w_{3} < 2(w_{3} - w_{4})$ and if both alternatives are picked as winners then $w_{2} - w_{3} = 2(w_{3} - w_{4})$. 

Nevertheless, in this example it is clear that $c$ will always be preferred to other alternatives (see Claim \ref{clm:wsequence} in Section \ref{sec:weights}). To see it we can express for each alternative the number of agents placing it at a given rank or at a higher one.

\[
\begin{array}{ccccc}
& \geq 1^\circ
& \geq 2^\circ
& \geq 3^\circ
& \geq 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 1
& 3 \\
b
& 0
& 0
& 3
& 3 \\
c
& 2
& 3
& 3
& 3 \\
d
& 0
& 1
& 2
& 3 \\
\end{array}
\]


\begin{claim}
	Consider a set $A$ of $m$ alternatives and let $r_k(i)$ be the number of times the alternative $i$ obtains a rank $k$ or a higher one. Consider two alternatives $ i,j \in A$, if $\forall \ k=2, \dots,m \ r_k(i)\geq r_k(j)$ and $r_1(i) > r_1(j)$ then $i$ is always preferred to $j$ for any choice of weights.
\end{claim}

\begin{proof}
	For our assumptions we know the sequence of weights is monotonic non-increasing and convex. Moreover, for the Claim \ref{clm:wsequence} in Section \ref{sec:weights}, we know that $w_1 > w_2$. Therefore, even in the worst case where $r_k(i) = r_k(j) \ forall \ k=2, \dots,m$ the sum of weights for alternative $i$ cannot be lower than the one for $j$. It is worth noting that we cannot say anything when $r_1(i) = r_1(j)$. Indeed, as said in Corollary \ref{cor:weq}, all the weights for other position but the first one can be equal, thus the number of agents ranking the alternatives at a certain position does not matter anymore.	
\end{proof}

So the strategy for querying the committee is to use two alternatives $i$ and $j$ to represent the query and complete the profile such that the other alternatives are all dominated by them. An algorithmic approach is to start from the initial profile and then add as many agents as needed that rank $i$ and $j$ as their top choices and the other alternatives afterwards. As an example let consider again the query \ref*{eqn:juryquery}. We want the committee to choose between $a$ or $b$:

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 0
& 1
& 0
& 2 \\
b
& 0
& 0
& 3
& 0 \\
\end{array}
\]

We add agents in order to increase the number of times $a$ and $b$ are ranked at first position. Please note we must maintain these scheme, so every add of a position to the ranking of $a$ must correspond to the same in the ranking of $b$.

\[
\begin{array}{cccccc}
v_1
& v_2
& v_3 
& v_4
& v_5
& v_6 \\
\midrule 
a
& b
& c 
& d
& a
& b \\
c
& a
& d
& c
& b
& a \\
b
& d
& b
& b
& d
& c \\
d
& c
& a
& a
& c
& d \\
\end{array}
\]

\[
\begin{array}{ccccc}
& 1^\circ
& 2^\circ
& 3^\circ
& 4^\circ \\
\cmidrule{2-5}
a 
& 2
& 2
& 0
& 2 \\
b
& 2
& 1
& 3
& 0 \\
c
& 1
& 2
& 1
& 2 \\
d
& 1
& 1
& 2
& 2 \\
\end{array}
\]

\[
\begin{array}{ccccc}
& \geq 1^\circ
& \geq 2^\circ
& \geq 3^\circ
& \geq 4^\circ \\
\cmidrule{2-5}
a 
& 2
& 4
& 4
& 6 \\
b
& 2
& 3
& 6
& 6 \\
c
& 1
& 3
& 4
& 6 \\
d
& 1
& 2
& 4
& 6 \\
\end{array}
\]
\textbf{Remarks:}
\begin{itemize}
	\item We are considering $\lambda \in \mathbb{N} \setminus \{0\}$ but we could be interested in a real number. \textit{Solution}: we can multiply both sides.
\end{itemize}
\fi
\end{document}  
