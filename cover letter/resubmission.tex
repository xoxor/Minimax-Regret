\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[version=3.21, pagesize, twoside=off, bibliography=totoc, DIV=calc, fontsize=12pt, a4paper]{scrartcl}
\input{preamble/packages}
\input{preamble/redac}
\input{preamble/math_basics}
\input{preamble/math_mine}
\usepackage{pdfpages}

%I find these settings useful in draft mode. Should be removed for final versions.
	%Which line breaks are chosen: accept worse lines, therefore reducing risk of overfull lines. Default = 200.
		\tolerance=2000
	%Accept overfull hbox up to...
		\hfuzz=2cm
	%Reduces verbosity about the bad line breaks.
		\hbadness 5000
	%Reduces verbosity about the underful vboxes.
		\vbadness=1300

\title{Resubmission: Simultaneous Elicitation of Scoring Rule and Agent Preferences for Robust Winner Determination}
\author{Submission ID: 554, AAMAS-2021}
\date{}
\hypersetup{
	pdfsubject={Cover letter},
	pdfkeywords={elicitation},
}

\begin{document}
\maketitle

\begin{abstract}
Social choice deals with the problem of determining a consensus choice from the preferences of different agents.
In the classical setting, the voting rule is fixed beforehand and full information concerning the preferences of the agents is provided.
This assumption of full preference information has recently been questioned by a number of researchers and
	several methods for eliciting the preferences of the agents have been proposed.
	%  are completely known have been proposed, as well as techniques that consider the opposite scenario. 

In this paper we argue that in many situations one should consider as well the voting rule to be 	partially specified.
%	This article goes one step further by tackling one important new case, namely, when both the voting rule and the agents preferences are partially known. It also permits to obtain a progressively refined recommendation without requiring to achieve full knowledge of either sort of information.
%This is useful because providing such information is often costly: the chair may find it hard to specify a rule completely; agents may need time to form reflective preferences.	
	Focusing on positional scoring rules, we assume that the chair, while not able to give a precise definition of the rule, is capable of answering simple questions requiring to pick a winner from a specific example profile. In addition, we assume that the agent preferences have to be elicited as well and are acquired by asking comparison queries. 
	We propose a method for robust approximate winner determination and interactive elicitation based on minimax regret; we develop several strategies for choosing the questions to ask to  the chair and the agents in order to %acquire the most relevant information for
converge quickly to a near-optimal alternative. Finally, we analyze these strategies in experiments %in order to validate our approach
 %focusing on settings 
 where the voting rule and the preferences are simultaneously elicited.
 %and obtain practical guidelines.
\end{abstract}

\section{Cover Letter}
Dear editors and reviewers,

We have submitted this article for the first time for IJCAI 2020. The reviewer’s evaluations of it were: from R1, Accept (good paper, I can argue for accepting it); from R2, Weak accept (marginally above the acceptance theshold, rejecting it would not be that bad); from R3, Weak reject (marginally below the acceptance threshold, accepting it would not be that bad); and from R4, Reject (Not good enough, I can argue for rejecting it). The strongest criticisms from the last two reviewers were, from R3, to improve presentation with a larger example and to expand the discussion (R3 incorrectly thought that we had room to spare within the constraints imposed by IJCAI); and from R4, that our literature review omitted important related works; that the results in the paper were incremental in nature; and that it was almost impossible to find the main contribution or take away message from the paper. The metareview pointed out that “One might view the work as somewhat incremental, but there is some substantial solid work here”; but recommended rejection because of 1) the very brief discussion of the empirical evaluation and 2) the incomplete literature review. The metareviewer also commented that “after some further work on the paper there's a reasonable chance that it could be accepted for another high quality conference.”

We extended very significantly the literature review and the experimental section, also taking advantage of two supplementary pages, and re-submitted the work for AAMAS 2021. The evaluations were: from R1 and R2, poor (4, good attempt but too many concerns, so probably should be rejected); and from R3, good (7, probably should be accepted). R1 considered our theoretical results as relatively simple extensions of the results of Lu and Boutilier and found the experimental results quite preliminary. About the first point, R1 stated to be unable to think about an example of a setting where our proposition would apply. After rebuttal, R1 stated that the example we provided in our response was convincing, but insisted that we need to test our heuristics beyond the impartial culture hypothesis. About experiments, R1 regretted that we use only impartial culture assumption; that we look at very small elections only; and that the way some of our conclusions are phrased make them appear more general than allowed by the experimental observations. R2 criticized the lack of theoretical guarantees for the proposed elicitation strategies, as well as the missing reasoning behind several of the experimental observations. After rebuttal, R2 took note of our observation that obtaining theoretical guarantees is generally considered beyond reach in a regret minimization setting; but insisted that experimental analysis should be presented by first formulating hypotheses to be tested, then showing how the experiments support or refute those hypotheses. Finally, R3, similarly to R1, found unclear which sorts of situations would permit an application of our proposed approach.

\section{Reviews AAMAS-2021}
\subsection*{Review 1}
\subsubsection*{Summary:}	The authors use the notion of minimax regret to guide the process of preference elicitation in an election (they elicit both the voters' preferences and the preferences of the chair regarding the scoring rule used). The authors show that the methodology of Lu and Boutilier for computing minimax regret still applies in their setting and present some experimental results.
\subsubsection*{Strengths:}	1) Minimax regret is a rather natural notion, which has not received much attention so far.
\subsubsection*{Weaknesses:}
1) The theoretical results are relatively simple extensions of the results of Lu and Boutilier
\newline 2) The experimental results seem quite preliminary.

\subsubsection*{Detailed comments:}	The paper is quite well written, clear, relevant for the conference. The biggest issues I have regard significance. The idea of using minimax regret to guide elicitation is very good, but is explored quite well by Lu and Boutilier. Here the authors extend this framework by eliciting the scoring rule used, but there is very little discussion as to why this really is such a good idea. I cannot easily think of a half-realistic setting where one would know that the scores need to be convex, but at the same time one would not know which ones exactly to use.

More specific comments come below:
\begin{enumerate}
\item Related work \\
As the paper is partially related to designing/choosing a scoring rule for a given setting---and as the problem of manipulation is mentioned---it might be worth to consider the following paper:

Dorothea Baumeister, Tobias Hogrebe:
Manipulative Design of Scoring Systems. AAMAS 2019: 1814-1816

Regarding partial preferences and manipulation, one may also look at the following one (although it is less related, for sure):

Dorothea Baumeister, Piotr Faliszewski, Jérôme Lang, Jörg Rothe:
Campaigns for lazy voters: truncated ballots. AAMAS 2012: 577-584


\item Concurrent elicitation of the scoring rule \\
I am not really convinced that the assumption that a scoring rule is not known makes sense. On the one hand, there is the problem of choosing the scoring rule manipulatively (but since the authors mention that the elections at hand are built into recommendation systems etc., this is not really an issue). The other problem is that I can hardly think of a scenario where the chair can answer the questions about the scoring rule, but somehow it is difficult for him to produce the full rule in a single step. So what do we gain?

\item Claims vs Propositions \\
I do not understand why the authors use "Claims" and not "Propositions".

\item I think that Claim 4 should say something along the lines: "There is a profile P', such that ... and P' can be computed effectively".

\item I can hardly imagine a person who faced with the profile P' from Claim 4 could meaningfully answer if a is better than b or the other way round. A piece of software could do it, though---but probably by precomputing the scoring rule to use (which removed the need for eliciation).

\item Proof of Claim 4: I do not understand notation (3,m) or (4,m-1) etc.

\item Empirical evaluation is very preliminary
I have a number of complaints regarding the evaluation process:
	\begin{enumerate}
		\item Why do the authors use IC only? Why not other statistical cultures? Why not some real-life data? After all, the experimental part is the main contribution of the paper and it is done in a fairly minimalistic way.
		
		\item Why do the authors look at very small elections only? 15 candidates and 30 voters seems completely inadequate for the motivations from the introduction.
		
		\item The conclusion that the number of questions required to reach low regret cannot be made based on Table 3. There is simply far too little data to have any sort of confidence in claims like this.	
	\end{enumerate}

\end{enumerate}

All in all, I think it is certainly a nice enough paper to be accepted as short, but is below the bar for the full acceptance.

\subsubsection*{Questions for rebuttal:}
Q1: Why did you only look at IC elections?
\newline Q2: How to use your approach in large elections? Hundreds of candidates, thousands of voters? After all, showing a strategy that can deal with large elections would meaningfully extend the work of Lu and Boutilier?
\newline Q3: Can you provide a realistic example where elicitation of the scoring rule is useful?
\subsubsection*{Score:}	
4: (poor (good attempt but too many concerns, so probably should be rejected))

\subsubsection*{Comments added after rebuttal:}
I found the example provided in the response to be convincing.

I still think that testing the heuristics on IC only is insufficient. Even if it is the hardest case, I would also like to see how the algorithm peforms in the easier ones. Can it exploit the structure when the structure is present? One cannot evaluate such issues by looking at IC alone.

\subsection*{Review 2}
\subsubsection*{Summary:}
The paper studies a voting problem wherein neither the voting rule nor the voters' preferences are specified in advance. Rather, an elicitation procedure is used to determine the election winner by asking the voters questions about their preferences and asking the chair questions about the voting rule.

Specifically, the preferences of the voters are assumed to be given by a fixed set of linear orders that are unknown to the elicitation platform. In addition, the voting rule is assumed to belong to the class of positional scoring rules with monotone weights with diminishing differences. The elicitation procedure comprises of the chair specifying the winning candidates on example profiles, and the voters responding to a series of pairwise comparisons.

At each step, the minimax regret criterion (Lu and Boutilier, IJCAI'11) is used to determine the next query, and this procedure is continued until convergence (e.g., either the regret drops below a certain threshold or a certain number of questions have been asked). The regret of an alternative, informally speaking, is the "loss" incurred for picking that alternative instead of an "optimal" one under adversarial choice of completion of the partial preferences elicited so far as well as the worst-case choice of positional weights.

The main contributions of the submission are:
1. Proposing a framework for simultaneously eliciting voters' preferences and information about the voting rule based on the minimax regret criterion.
2. Experimental comparison of a number of elicitation heuristics in terms of the number of questions asked or the regret value.

The empirical observations suggest that a modest number of questions (to the voters as well as the chair) are sufficient in arriving at a reasonable choice, and that the regret drops super-linearly as a function of the number of questions asked over the course of elicitation.
\subsubsection*{Strengths:}	The idea of modeling simultaneous uncertainty in the voting rule as well as the voters' preferences is quite interesting. It is also useful to know that the minimax regret framework of Lu and Boutilier (IJCAI'11), previously studied in the setting of a known voting rule, can be extended to the aforementioned more general problem.
\subsubsection*{Weaknesses:}	The lack of theoretical guarantees for the proposed elicitation strategy, as well as missing reasoning behind several of the experimental observations, makes this a weak paper in my opinion.

\subsubsection*{Detailed comments:}
I like the high-level question asked by the paper, namely whether it is possible to simultaneously elicit the information about the voting rule and the voters' preferences. However, the significance of the contribution is less clear. The results, which largely comprise of experiments, are at the level of proposing a heuristic that seems to work well in simulations but lacks formal theoretical guarantees.

I believe the paper will be much stronger if the authors could provide theoretical convergence guarantees for the proposed heuristics. Additionally, in the "Empirical Evaluation" section, instead of simply describing the observations, it would be useful to also formally state the hypotheses used in each experiment, and explain how the experiments support or refute those hypotheses. For example, you could describe whether (and why) one might expect "Pessimistic" strategy to work better than the "Elitist" strategy, and then discuss to what extent did the simulations confirm that hypothesis.

Some minor comments are below.
\begin{enumerate}
\item Page 1
\begin{itemize}
	\item In several places you use the phrase "permits to" when it might be clearer to say "allows for".
\end{itemize}

\item Page 2
\begin{itemize}
	\item "that showed that"-->who showed that.
\end{itemize}

\item Page 3 
\begin{itemize}
	\item "there are no necessary winner"-->there is no necessary winner.
\end{itemize}

\item Page 4 
\begin{itemize}
	\item "less good than or equal to"-->You could rather say "weakly worse".
	\item Using $\succeq^p_j(x)$ to denote the set of alternatives weakly worse than x can be confusing. Please consider using a different notation such as $A^(\succeq^p_j,x)$.
\end{itemize}

\item Page 5
\begin{itemize}
	\item Claim 4 is written in the form of a definition, and it is not clear what is actually being claimed here. Maybe you could say that "there exists a profile P' satisfying such and such properties".
	\item "each of the p+q agent"-->each of the p+q agents
	\item "worst than"-->worse than
\end{itemize}

\item Page 6 
\begin{itemize}
	\item "on order to"-->in order to
	\item "an important weight"-->a large weight
	\item "For generating the weights we first draw $m - 1$ numbers uniformly at random"-->What is the range from which you make the draws?
\end{itemize}

\item Page 7 
\begin{itemize}
	\item "In-dept evaluation"-->In-depth evaluation
	\item Figure 1, X axis label should say "Number of questions"
	\item "to be ran"-->to be run
	\item "We see that the number of questions required to reach a low regret level grows approximately linearly with the number of agents"-->Can you provide a formal regression analysis to support this claim?
\end{itemize}

\item Page 8 
\begin{itemize}
	\item "allow to quickly"-->allows to quickly
	\item "worst regret"-->worst-case regret
	\item "that allows to reproduce our experiments, and more"-->It would be useful to mention the other benefits explicitly.
	\item "halving the number of questions will only reduce the gain in regret by less than half"-->Might be clearer to say "halving the number of questions reduces the regret by more than 50$\%$".
\end{itemize}

\end{enumerate}
\subsubsection*{Questions for rebuttal:}	1. On Page 1, you write that "The quality of the recommendation increases faster than linearly with the number of questions, after an initial phase with almost no increase in quality, before slowing down and converging to an optimal recommendation." What is the reasoning behind this trend?
\newline 2. How did you decide the range of the parameter $\lambda$ to be [1,n]?
\subsubsection*{Score:}	
4: (poor (good attempt but too many concerns, so probably should be rejected))

\subsubsection*{Comments added after rebuttal:}
Thanks to the authors for responding to the questions raised in my review. My concern about the significance of the paper remains. In the absence of any formal theoretical guarantees about the performance of the elicitation heuristic, the paper will likely be of limited interest. The authors note that obtaining such guarantees could be difficult. Nevertheless, the paper could certainly benefit from a revised experimental analysis that clearly formulates the hypotheses to be tested and how the experiments support or refute those hypotheses.

\subsection*{Review 3}
\subsubsection*{Summary:}
This paper considers elicitation in the context of scoring rules when both the preferences of the voters and the weights defining the rule might be missing. The authors model the problem via regret minimization and propose several types of queries and querying strategies. The paper also presents an interesting experimental study which supports the presented approach.
\subsubsection*{Strengths:}	The problem addressed is interesting and has potential for practical applicability.
Addressing missing information in both voters and the chair’s preferences is novel. The experimental section is a good starting point.
\subsubsection*{Weaknesses:}	The discussion on the motivation behind considering uncertainty on the scoring rule’s weights is not very convincing. They claim the chair might not be able to formalize the rule…
Another point that is not clear is when, in practice a system like this would be deployed.
The application of regret minimization is somewhat incremental w.r.t. the literature.

\subsubsection*{Detailed comments:}	reference elicitation in the context of voting is certainly a relevant topic for AAMAS.
The setting considered here is not per se novel, as elicitation of voters’ and the chair’s preferences have been considered before. The originality lies in considering them both at the same time.
While this is original, the motivation of when such a setting may occur in practice is not clear. In particular, I wonder about the timing. It is assumed that voters and candidates are known, information about the voters preferences is available and yet the voting rule has not yet been decided and the elicitation process is going on at the same time. It seams unlikely to me.
As I mentioned, I am also not very convinced about a chair not knowing which rule he wants/or is using while voters are expressing preferences. A much stronger argument would be that of a third party discovering information about a voter’s preferences and the rule by being able to observe results or to tap in and acquire some information. Obviously that would mean having much less control over what information is acquired than assumed here…it might be similar to random strategy studied in the experimental section.

The content is technically not overly sophisticated, so I have no doubt about its soundness.
The presentation flows well and all parts are clearly written. The description of the experimental results is also sufficiently detailed to enable reproducibility.

A minor complaint is that Figures 1 and 2 are blurred.
\subsubsection*{Questions for rebuttal:}
1) Could testing mixed strategies yield better results than the pure ones?
\subsubsection*{Score:}	
7: (good (probably should be accepted))

\subsection*{Metareview}
The main reason for the reject recommendation is the missing novelty in this paper. It basically is a reimplementation of the ideas from Lu and Boutilier. Furthermore, in the experimental section, clear hypotheses are missing and the reasons for limiting the analysis to IC are not convincing.

\subsection{Our rebuttal}
Our proposal is methodologically innovative: it addresses in a new way the difficult problem of eliciting preferences about voting rules. It opens up a novel path to gather and exploit preferential information about voting rules, in parallel with more classical preferential information about the alternatives under scrutiny. As a result, multiple challenging research questions open up, several of which are raised by the reviewers. We warmly thank them for these observations.

As reviewer 1 points out, one could simply ask for direct information about the parameters of the rules, such as the weight vector. But it is unclear whether the answers to such questions would be meaningful. For example, in many cases, the chair (supposed to be human) may not know whether the largest weight should be twice, or three times, the weight associated with the second rank.

We ask the chair which alternative should win given a concrete profile. Such questions have precise semantics whose understanding is shared by the chair. This way of asking questions is sufficiently natural that it has been used in experimental settings investigating the feeling of justice of individuals (Does majoritarian approval matter in selecting a social choice rule? An exploratory panel study, Ebru Giritligil and Sertel, Social Choice and Welfare 25(1):43-73, 2005). To the best of our knowledge, the use of such questions to systematically guide an elicitation process is novel. We will highlight this aspect in the paper.

Our approach is analogous to that of decision theorists with preference elicitation: a large stream of research considers more meaningful to ask direct choice questions ("please choose either a or b") than to question the decision maker about the shape of its utility function. The former are considered "observable": acts of choice are translated to preference statements.

Concerning the hypothesis of convexity raised by reviewer 1, we need to assume that we look for a function inside a specific class of functions (here, the scoring rules with convex weights) because the class of all possible functions is too large. This allows for mixing an axiomatic basis with more precise "observable" questions; again, as it is usually done in decision theory (which often assumes the Subjective Expected Utility axioms). The use of convex weights is a natural and common assumption: losing ranks at the top of the ranking is more damaging than losing ranks at the bottom.

A concrete application of our work might be a committee that is about to hire a new employee, whose performances are evaluated by several experts. The members of the committee may not have a voting rule in mind at the start of the process, and might not wish to agree on a specific voting rule, but they might be willing to answer enough questions about selecting winners out of specific profiles to determine their employee of choice.

Regarding the experimental concerns addressed by reviewer 1, our strategies and setting are meant to tackle small scale social choice situations such as just exemplified. Our questioning method, in particular, does not scale to hundreds of candidates because the chair must be assumed to have limited cognitive capacity.

IC is a quite challenging situation: as can be expected intuitively (and shown experimentally by Lu and Boutilier in their setting), the number of questions to be asked decrease with less varied profiles. We want to focus on the most critical setting, as there is a wide variety of possible situations to test in such experiments.

The reviewers are right to point out that our comments about Table 3 were overconfident, this will be corrected.

Reviewer 1 Question6 : We should have written “given to c the ranks 3 and m” instead of (3, m) in the proof of Claim 4.

Reviewer 2 Question2: The $\lambda$-range is initialized to [1,n] because $\lambda \geq1$ by convexity and we assume $\lambda \leq n$ wlog: for $\lambda \geq n$, the rule coincides with Plurality.

Regarding the guarantees concerns raised by reviewer 2: The general lack of theoretical guarantees with respect to the number of questions in the literature adopting minimax regret approaches suggest that obtaining such results is very difficult, if possible. However, our approach guarantees to return the true winners when the elicitation is completed. Note that the process can be terminated early, and offers a guaranteed bound on the regret at any time.

Reviewer 2 Question1: At the start of the elicitation, the space of uncertainty is so large that the adversary can attain the maximum possible value of regret; multiple answers are needed to constrain her enough for the MMR to start decreasing. When approaching optimality, the pace of MMR reduction decreases.

Reviewer 3 Question1: We did not investigate a systematic way of mixing strategies but we manually mixed different ideas in order to obtain the strategies we presented. For example Two phase and Extended Pessimistic reuse ideas present in the Pessimistic strategy.

\section{Reviews IJCAI-20}
\input{reviews_ijcai.tex}

\section{Rejected paper AAMAS-2021}
%\includepdf[pages=-]{aamas21_paper_554.pdf}

%\bibliography{bibl}

\end{document}

